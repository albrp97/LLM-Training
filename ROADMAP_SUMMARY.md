# Research Roadmap - Resumen Ejecutivo

## ðŸŽ¯ Estado Actual

**3 Fases Completadas** | **12 Modelos Evaluados** | **3 Papers Documentados**

### Mejores Modelos por CategorÃ­a

| CategorÃ­a | Modelo | MÃ©trica Clave | Por QuÃ© |
|-----------|--------|---------------|---------|
| ðŸ† **Overall Best** | 4B-SFT (NoQuant) | AbsDiff 9.0, 7.87GB | Ãšnico que funciona en TODOS los dominios |
| ðŸ§  **OOD Reasoning** | 8B-DoRA (NoQuant) | F1 0.8995 | Mejor generalizaciÃ³n fuera de dominio |
| ðŸ’» **Consumer GPU** | 8B-QLoRA | 7.6GB training | 62% menos VRAM que LoRA estÃ¡ndar |
| ðŸ’° **Low VRAM Training** | 8B-VeRA | 16.2GB training | 30-41% menos que LoRA/DoRA |

---

## ðŸš¨ Top 10 Gaps Identificados

1. **âŒ Escalado intermedio**: Salto de 8B â†’ 70B sin explorar 14B-30B
2. **âŒ QAT**: Post-Training Quantization falla, necesitamos Quantization-Aware Training
3. **âŒ PEFT avanzado**: AdaLoRA, LoRA+, Q-Adapter sin evaluar
4. **âŒ Multi-task**: Solo single-task SFT, falta multi-domain training
5. **âŒ Dominios limitados**: Solo Math/QA/MCQ, falta code/chat/domain-specific
6. **âŒ HiperparÃ¡metros**: Single-seed, no sweeps, hiperparÃ¡metros no optimizados
7. **âŒ Inference**: Latency medida pero no optimizada (vLLM, speculative decoding)
8. **âŒ Arquitectura Ãºnica**: Solo Qwen3, falta Llama/Mistral/MoE
9. **âŒ MÃ©tricas limitadas**: Solo accuracy, falta throughput/cost/calibration
10. **âŒ Data efficiency**: Fixed 1000 samples, no few-shot ni active learning

---

## ðŸ—ºï¸ Roadmap Priorizado (6 Meses)

### **FASE 4: QuantizaciÃ³n Avanzada** (Meses 1-2) ðŸ”¥
**Objetivo**: Resolver problema PTQ y encontrar mejor estrategia de quantizaciÃ³n

**Experimentos Clave**:
- âœ… QAT (Quantization-Aware Training) vs PTQ head-to-head
- âœ… SmoothQuant, AWQ, GPTQ implementation y comparison
- âœ… Mixed-precision strategies (diferentes precisiones por capa)

**Entregable**: Paper "Advanced Quantization for Fine-Tuned LLMs"  
**Hardware**: RTX 4090 (24GB) suficiente  
**Impact**: CRÃTICO - PTQ actual es catastrÃ³fica para modelos fine-tuned

---

### **FASE 5: Escalado Intermedio** (Meses 3-4) ðŸ”¥
**Objetivo**: Llenar gap 8B-70B con modelos 14B

**Experimentos Clave**:
- âœ… Qwen3-14B full benchmark (base, SFT, LoRA, DoRA, VeRA, QLoRA)
- âœ… Scaling laws analysis (1.7B â†’ 4B â†’ 8B â†’ 14B)
- âœ… Â¿14B-QLoRA supera a 8B-DoRA? Â¿14B-SFT es viable en consumer?

**Entregable**: "Scaling Laws for Quantized PEFT: 1.7B to 14B"  
**Hardware**: 2Ã— RTX 4090 (48GB) o A100 40GB recomendado  
**Impact**: ALTO - Define si escalar vale la pena para consumer GPUs

---

### **FASE 7: OptimizaciÃ³n de Inference** (Meses 5-6)
**Objetivo**: Reducir latency sin sacrificar quality

**Experimentos Clave**:
- âœ… KV-cache quantization (4-bit, 8-bit)
- âœ… Speculative decoding (draft: 1.7B, target: 8B-DoRA)
- âœ… Framework comparison (vLLM, TensorRT-LLM, llama.cpp)

**Entregable**: "Production-Ready Inference Optimization"  
**Hardware**: RTX 4090 suficiente  
**Impact**: MEDIO-ALTO - CrÃ­tico para deployment en producciÃ³n

---

## âš¡ Quick Wins (PrÃ³ximas 2 Semanas)

### **Semana 1: ValidaciÃ³n y MÃ©tricas**
1. **Multi-seed validation** (2 dÃ­as)
   - 3 seeds de 4B-SFT-OpenMath
   - Establecer error bars
   
2. **MÃ©tricas adicionales** (1 dÃ­a)
   - Throughput (tokens/sec)
   - Cost-per-token estimates
   
3. **Hyperparameter sweep limitado** (2 dÃ­as)
   - Learning rates: 1e-5, 5e-5, 1e-4
   - Solo en 4B-SFT-OpenMath

### **Semana 2: Proof-of-Concept FASE 4**
4. **QAT implementation bÃ¡sica** (3 dÃ­as)
   - Fork `01_Train.py` â†’ `01_Train_QAT.py`
   - Integrar Hugging Face QAT
   
5. **QAT vs PTQ experiment** (2 dÃ­as)
   - 4B-QAT-OpenMath vs 4B-PTQ-OpenMath
   - Si QAT funciona â†’ full FASE 4

---

## ðŸ“Š Decisiones CrÃ­ticas

### **Pregunta 1: Â¿QuÃ© fase priorizar?**

**OpciÃ³n A: FASE 4 (QuantizaciÃ³n)** â­ RECOMENDADO
- âœ… Mayor impacto inmediato
- âœ… PTQ demostrÃ³ ser problemÃ¡tica
- âœ… Hardware actual suficiente
- âœ… Resultados aplicables a TODAS las escalas

**OpciÃ³n B: FASE 5 (Escalado 14B)**
- âš ï¸ Requiere mÃ¡s hardware (48GB+)
- âš ï¸ Puede no agregar mucho vs 8B
- âœ… Completa la curva de escalado
- âœ… Valida patterns en tamaÃ±os mayores

**RecomendaciÃ³n**: **FASE 4 primero**, luego FASE 5 si hardware lo permite

---

### **Pregunta 2: Â¿ExpansiÃ³n o ProfundizaciÃ³n?**

**Profundizar (RECOMENDADO para paper de calidad)**:
- Resolver gaps en configuraciones actuales
- Multi-seed, sweeps, optimizaciÃ³n
- Mejor caracterizaciÃ³n de trade-offs

**Expandir (mejor para cobertura)**:
- Nuevos datasets (code, chat)
- Nuevas arquitecturas (Llama, Mistral)
- Multi-task experiments

**RecomendaciÃ³n**: **70% Profundizar, 30% Expandir**

---

### **Pregunta 3: Â¿Target de PublicaciÃ³n?**

**OpciÃ³n A: Top Conference (ICML, NeurIPS, ICLR)**
- Requiere: 6-8 meses trabajo, resultados muy sÃ³lidos
- Necesita: Multi-seed, ablations completas, scaling laws
- Fases necesarias: 4 + 5 + validaciÃ³n exhaustiva

**OpciÃ³n B: Workshop / Technical Report**
- Requiere: 3-4 meses trabajo
- Menos riguroso pero mÃ¡s rÃ¡pido
- Fases necesarias: 4 o 5 (una de las dos)

**OpciÃ³n C: Blog Posts + Open Research**
- Publicar hallazgos continuamente
- Community engagement
- Fases: Iterativo, cada fase = post

**RecomendaciÃ³n**: **OpciÃ³n C + apuntar a Workshop** (mÃ¡s impacto prÃ¡ctico)

---

## ðŸŽ¯ AcciÃ³n Inmediata (Hoy/MaÃ±ana)

### **HOY**
```bash
# 1. Revisar roadmap y decidir prioridad
# 2. Setup multi-seed experiment
cp Fine-tuning/01_Train.py experiments/01_Train_MultiSeed.py
# Modificar para iterar seeds 42, 43, 44
```

### **ESTA SEMANA**
```bash
# 3. Implementar QAT bÃ¡sica
# 4. Ejecutar 4B-QAT-OpenMath
# 5. Comparar vs 4B-PTQ-OpenMath
```

### **PRÃ“XIMAS 2 SEMANAS**
```bash
# 6. Si QAT funciona â†’ commit a FASE 4 completa
# 7. Si QAT falla â†’ considerar FASE 5 (14B)
# 8. Implementar mÃ©tricas adicionales (throughput, cost)
```

---

## ðŸ“ˆ KPIs de Ã‰xito

### **Objetivos TÃ©cnicos (3 meses)**
- [ ] MÃ©todo de quantizaciÃ³n que preserve >95% accuracy con <50% VRAM
- [ ] Demostrar clara ventaja (o diminishing returns) de 14B vs 8B
- [ ] Lograr â‰¥2Ã— speedup en inference vs baseline actual

### **Objetivos de InvestigaciÃ³n (6 meses)**
- [ ] 2 papers/reports publicados
- [ ] Principios generalizables mÃ¡s allÃ¡ de Qwen3
- [ ] Recetas claras para practitioners

### **Objetivos de Impacto (12 meses)**
- [ ] Fine-tuning de calidad en GPUs <$2000
- [ ] Citaciones / uso en comunidad
- [ ] Contribuciones upstream a PEFT/Transformers

---

## ðŸ’¡ Ideas Exploratorias (Moonshots)

Si tiempo/recursos sobran:

1. **LoRA Surgery**: Â¿Transfer adapters entre tamaÃ±os de modelo?
2. **QA-LoRA**: Combinar QAT con PEFT training
3. **Dynamic Rank**: Diferentes ranks por capa (auto-search)
4. **Hybrid Precision**: Batch (4-bit) vs Online (8-bit) serving

---

## ðŸ¤ Recursos Necesarios

| Item | MÃ­nimo | Ã“ptimo | Ideal |
|------|--------|--------|-------|
| **GPU** | 1Ã— RTX 4090 24GB | 2Ã— RTX 4090 48GB | 1Ã— A100 40GB |
| **Permite** | Hasta 8B-QLoRA | 14B-QLoRA | 14B-SFT, 32B-QLoRA |
| **Tiempo** | 6 meses (1 persona) | 3-4 meses (2 personas) | 2-3 meses (team) |
| **Costo aprox.** | $0 (ya tienes) | $1500-2000 | $20k/aÃ±o cloud |

---

## âœ… RecomendaciÃ³n Final

**PrÃ³ximos 2-3 meses**: 
1. âœ… **FASE 4 (QuantizaciÃ³n)** - Mayor impacto, hardware actual suficiente
2. âœ… ValidaciÃ³n multi-seed de resultados actuales
3. âœ… QAT proof-of-concept esta semana

**Meses 4-6**: 
1. âš¡ **FASE 5 (14B)** si hardware permite, o
2. âš¡ **FASE 7 (Inference)** si quieren deployment focus

**PublicaciÃ³n**:
- Target: Workshop paper en 4 meses
- Continuous blog posts
- Open-source todos los scripts

**ROI**: Maximiza impacto cientÃ­fico Y valor prÃ¡ctico para la comunidad ðŸš€

---

**Siguiente reuniÃ³n**: Decidir FASE 4 vs FASE 5, discutir hardware upgrade

