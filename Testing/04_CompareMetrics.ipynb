{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d3bf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Wrote summary with shape (6, 11) to: metrics_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_peak_vram_reserved_gb</th>\n",
       "      <th>train_peak_vram_allocated_gb</th>\n",
       "      <th>eval_peak_vram_reserved_gb</th>\n",
       "      <th>eval_peak_vram_allocated_gb</th>\n",
       "      <th>ai2_arc__macro_f1</th>\n",
       "      <th>ai2_arc__latency_mean_s</th>\n",
       "      <th>OpenMathInstruct-2__avg_abs_diff</th>\n",
       "      <th>OpenMathInstruct-2__latency_mean_s</th>\n",
       "      <th>squad_v2__F1</th>\n",
       "      <th>squad_v2__latency_mean_s</th>\n",
       "      <th>OpenMathInstruct-2__accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Models__Qwen3-0.6B-base</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.375</td>\n",
       "      <td>1.280</td>\n",
       "      <td>0.5111</td>\n",
       "      <td>1.7676</td>\n",
       "      <td>2294.642941</td>\n",
       "      <td>7.3468</td>\n",
       "      <td>10.33</td>\n",
       "      <td>0.2334</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Models__Qwen3-0.6B-base_gptq_w4g64</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.434</td>\n",
       "      <td>1.323</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>29.6305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.6095</td>\n",
       "      <td>50.00</td>\n",
       "      <td>29.8376</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant</th>\n",
       "      <td>5.688</td>\n",
       "      <td>5.346</td>\n",
       "      <td>1.252</td>\n",
       "      <td>1.219</td>\n",
       "      <td>0.2143</td>\n",
       "      <td>0.0886</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>10.33</td>\n",
       "      <td>0.2466</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant_gptq_w4g64</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.434</td>\n",
       "      <td>1.323</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>30.1670</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.7323</td>\n",
       "      <td>50.00</td>\n",
       "      <td>29.3290</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen3-0.6B-openmath_SFT_LoRa64_NoQuant</th>\n",
       "      <td>2.814</td>\n",
       "      <td>2.188</td>\n",
       "      <td>1.375</td>\n",
       "      <td>1.284</td>\n",
       "      <td>0.6610</td>\n",
       "      <td>1.8407</td>\n",
       "      <td>137.154211</td>\n",
       "      <td>3.6479</td>\n",
       "      <td>10.33</td>\n",
       "      <td>0.2360</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen3-0.6B-openmath_SFT_LoRa64_QLORA_w4_headbf16</th>\n",
       "      <td>1.877</td>\n",
       "      <td>1.398</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.4937</td>\n",
       "      <td>0.6717</td>\n",
       "      <td>50656.786500</td>\n",
       "      <td>8.3269</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3752</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    train_peak_vram_reserved_gb  \\\n",
       "model                                                                             \n",
       "Models__Qwen3-0.6B-base                                                   0.000   \n",
       "Models__Qwen3-0.6B-base_gptq_w4g64                                          NaN   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant                            5.688   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant_...                          NaN   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_NoQuant                                    2.814   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_QLORA_w4_headbf16                          1.877   \n",
       "\n",
       "                                                    train_peak_vram_allocated_gb  \\\n",
       "model                                                                              \n",
       "Models__Qwen3-0.6B-base                                                    0.000   \n",
       "Models__Qwen3-0.6B-base_gptq_w4g64                                           NaN   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant                             5.346   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant_...                           NaN   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_NoQuant                                     2.188   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_QLORA_w4_headbf16                           1.398   \n",
       "\n",
       "                                                    eval_peak_vram_reserved_gb  \\\n",
       "model                                                                            \n",
       "Models__Qwen3-0.6B-base                                                  1.375   \n",
       "Models__Qwen3-0.6B-base_gptq_w4g64                                       1.434   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant                           1.252   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant_...                       1.434   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_NoQuant                                   1.375   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_QLORA_w4_headbf16                         1.000   \n",
       "\n",
       "                                                    eval_peak_vram_allocated_gb  \\\n",
       "model                                                                             \n",
       "Models__Qwen3-0.6B-base                                                   1.280   \n",
       "Models__Qwen3-0.6B-base_gptq_w4g64                                        1.323   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant                            1.219   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant_...                        1.323   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_NoQuant                                    1.284   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_QLORA_w4_headbf16                          0.638   \n",
       "\n",
       "                                                    ai2_arc__macro_f1  \\\n",
       "model                                                                   \n",
       "Models__Qwen3-0.6B-base                                        0.5111   \n",
       "Models__Qwen3-0.6B-base_gptq_w4g64                             0.0000   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant                 0.2143   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant_...             0.0000   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_NoQuant                         0.6610   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_QLORA_w4_headbf16               0.4937   \n",
       "\n",
       "                                                    ai2_arc__latency_mean_s  \\\n",
       "model                                                                         \n",
       "Models__Qwen3-0.6B-base                                              1.7676   \n",
       "Models__Qwen3-0.6B-base_gptq_w4g64                                  29.6305   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant                       0.0886   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant_...                  30.1670   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_NoQuant                               1.8407   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_QLORA_w4_headbf16                     0.6717   \n",
       "\n",
       "                                                    OpenMathInstruct-2__avg_abs_diff  \\\n",
       "model                                                                                  \n",
       "Models__Qwen3-0.6B-base                                                  2294.642941   \n",
       "Models__Qwen3-0.6B-base_gptq_w4g64                                               NaN   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant                              6.000000   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant_...                               NaN   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_NoQuant                                    137.154211   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_QLORA_w4_headbf16                        50656.786500   \n",
       "\n",
       "                                                    OpenMathInstruct-2__latency_mean_s  \\\n",
       "model                                                                                    \n",
       "Models__Qwen3-0.6B-base                                                         7.3468   \n",
       "Models__Qwen3-0.6B-base_gptq_w4g64                                             29.6095   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant                                  0.0435   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant_...                             29.7323   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_NoQuant                                          3.6479   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_QLORA_w4_headbf16                                8.3269   \n",
       "\n",
       "                                                    squad_v2__F1  \\\n",
       "model                                                              \n",
       "Models__Qwen3-0.6B-base                                    10.33   \n",
       "Models__Qwen3-0.6B-base_gptq_w4g64                         50.00   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant             10.33   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant_...         50.00   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_NoQuant                     10.33   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_QLORA_w4_headbf16            0.00   \n",
       "\n",
       "                                                    squad_v2__latency_mean_s  \\\n",
       "model                                                                          \n",
       "Models__Qwen3-0.6B-base                                               0.2334   \n",
       "Models__Qwen3-0.6B-base_gptq_w4g64                                   29.8376   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant                        0.2466   \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant_...                   29.3290   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_NoQuant                                0.2360   \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_QLORA_w4_headbf16                      0.3752   \n",
       "\n",
       "                                                    OpenMathInstruct-2__accuracy  \n",
       "model                                                                             \n",
       "Models__Qwen3-0.6B-base                                                      NaN  \n",
       "Models__Qwen3-0.6B-base_gptq_w4g64                                           0.0  \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant                               NaN  \n",
       "Models__Qwen3-0.6B-openmath_SFT_NoPeft_NoQuant_...                           0.0  \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_NoQuant                                       NaN  \n",
       "Qwen3-0.6B-openmath_SFT_LoRa64_QLORA_w4_headbf16                             NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "# We scan both: Testing/metrics (current folder) AND repo-root/metrics\n",
    "METRIC_DIRS = [Path(\"metrics\"), Path(\"../metrics\")]\n",
    "OUT_CSV = Path(\"metrics_summary.csv\")\n",
    "DEBUG = False  # set True to print debug info\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def dataset_slug(name: str) -> str:\n",
    "    n = Path(name).stem\n",
    "    if n.startswith(\"test-\"):\n",
    "        n = n[5:]\n",
    "    return n\n",
    "\n",
    "def get_metric(ds_block: dict, key: str):\n",
    "    val = ds_block.get(key)\n",
    "    if val is None:\n",
    "        val = (ds_block.get(\"metrics\") or {}).get(key)\n",
    "    return val\n",
    "\n",
    "EFFICACY_POLICY = {\n",
    "    \"mcq4\": [\"macro_f1\", \"accuracy\"],\n",
    "    \"boolq\": [\"macro_f1\", \"MCC\", \"balanced_accuracy\", \"accuracy\"],  # excluded anyway\n",
    "    \"squad_v2\": [\"F1\", \"EM\"],\n",
    "    \"math_numeric\": [\"avg_abs_diff\"],  # lower is better\n",
    "}\n",
    "\n",
    "def choose_efficacy_metric(ds_block: dict) -> Optional[Tuple[str, float]]:\n",
    "    kind = (ds_block.get(\"type\") or \"\").lower()\n",
    "    for k in EFFICACY_POLICY.get(kind, []):\n",
    "        v = get_metric(ds_block, k)\n",
    "        if v is not None:\n",
    "            return k, v\n",
    "    for k in [\"accuracy\", \"macro_f1\", \"F1\", \"balanced_accuracy\", \"MCC\", \"EM\", \"avg_abs_diff\"]:\n",
    "        v = get_metric(ds_block, k)\n",
    "        if v is not None:\n",
    "            return k, v\n",
    "    return None\n",
    "\n",
    "def parse_model_info(model_id: str) -> dict:\n",
    "    m_ds = re.search(r'-([A-Za-z0-9_]+)_SFT_', model_id)\n",
    "    dataset = (m_ds.group(1).lower() if m_ds else None)\n",
    "\n",
    "    is_base = 1 if model_id.endswith('_base') else 0\n",
    "    if is_base:\n",
    "        dataset = '_base'\n",
    "\n",
    "    is_nopeft = 1 if 'NoPeft' in model_id else 0\n",
    "\n",
    "    m_lora = re.search(r'[Ll]ora(\\d+)', model_id)\n",
    "    lora_rank = int(m_lora.group(1)) if m_lora else -1\n",
    "    has_lora = 1 if m_lora else 0\n",
    "\n",
    "    dataset_order = 1 if dataset == '_base' else 0\n",
    "    if has_lora:\n",
    "        lora_group = 0\n",
    "    elif is_nopeft:\n",
    "        lora_group = 1\n",
    "    else:\n",
    "        lora_group = 2\n",
    "\n",
    "    m_size = re.search(r'-(\\d+(?:\\.\\d+)?)B-', model_id)\n",
    "    size_num = float(m_size.group(1)) if m_size else 0.0\n",
    "\n",
    "    return {\n",
    "        \"dataset\": dataset or \"\",\n",
    "        \"dataset_order\": dataset_order,\n",
    "        \"lora_group\": lora_group,\n",
    "        \"lora_rank\": lora_rank,\n",
    "        \"is_nopeft\": is_nopeft,\n",
    "        \"is_base\": is_base,\n",
    "        \"size_num\": size_num,\n",
    "    }\n",
    "\n",
    "def _gather_eval_jsons(dirs: List[Path]) -> List[Path]:\n",
    "    files: List[Path] = []\n",
    "    for d in dirs:\n",
    "        if d.exists():\n",
    "            for f in d.rglob(\"*.json\"):\n",
    "                if f.name == \"training_metadata.json\":\n",
    "                    continue\n",
    "                files.append(f)\n",
    "    files = sorted(set(f.resolve() for f in files))\n",
    "    if DEBUG:\n",
    "        print(f\"[DEBUG] Found {len(files)} evaluation JSON(s):\")\n",
    "        for f in files:\n",
    "            print(\"   \", f)\n",
    "    return files\n",
    "\n",
    "def _collect_models_roots() -> List[Path]:\n",
    "    \"\"\"\n",
    "    Find any 'Models' directory starting from CWD and walking up parents.\n",
    "    Running from Testing/, this yields '../Models' (and higher if needed).\n",
    "    \"\"\"\n",
    "    roots: List[Path] = []\n",
    "    here = Path.cwd().resolve()\n",
    "    for base in [here] + list(here.parents):\n",
    "        cand = base / \"Models\"\n",
    "        if cand.exists() and cand.is_dir():\n",
    "            roots.append(cand.resolve())\n",
    "    # Dedup, keep order\n",
    "    out, seen = [], set()\n",
    "    for r in roots:\n",
    "        if r not in seen:\n",
    "            seen.add(r)\n",
    "            out.append(r)\n",
    "    if DEBUG:\n",
    "        print(\"[DEBUG] Models roots:\", out)\n",
    "    return out\n",
    "\n",
    "def _load_training_vram_from_path(p: Path) -> Optional[Tuple[float, float]]:\n",
    "    try:\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            tm = json.load(f)\n",
    "        v = ((tm.get(\"hardware_info\") or {}).get(\"vram_peaks\")) or {}\n",
    "        res = v.get(\"overall_max_reserved_gb\")\n",
    "        alloc = v.get(\"overall_max_allocated_gb\")\n",
    "        if (res is not None) or (alloc is not None):\n",
    "            return res, alloc\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _safe_name(s: str) -> str:\n",
    "    return re.sub(r'[<>:\"/\\\\|?*\\x00-\\x1F]', \"_\", s)\n",
    "\n",
    "def _find_training_metadata(model_name_in_json: Optional[str], json_stem: str) -> Optional[Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Resolve training_metadata.json robustly when running from Testing/.\n",
    "    We try:\n",
    "      1) Direct model_name path (absolute or relative) + parents\n",
    "      2) <any Models root>/<basename(model_name)>/training_metadata.json\n",
    "      3) <any Models root>/<json_stem>/training_metadata.json\n",
    "      4) Fallback: search under each Models root for a folder whose name equals\n",
    "         json_stem or the model_info.model_name inside the file.\n",
    "    \"\"\"\n",
    "    models_roots = _collect_models_roots()\n",
    "\n",
    "    # 1) Direct model_name path tries (absolute or relative under cwd & parents)\n",
    "    cand_paths: List[Path] = []\n",
    "    if model_name_in_json:\n",
    "        p = Path(model_name_in_json)\n",
    "        # absolute as given\n",
    "        cand_paths.append(p / \"training_metadata.json\")\n",
    "        # relative under cwd and parents\n",
    "        here = Path.cwd().resolve()\n",
    "        for base in [here] + list(here.parents):\n",
    "            cand_paths.append((base / p / \"training_metadata.json\"))\n",
    "\n",
    "    # 2) Models roots + basename(model_name)\n",
    "    if model_name_in_json:\n",
    "        base_name = Path(model_name_in_json).name\n",
    "        for root in models_roots:\n",
    "            cand_paths.append(root / base_name / \"training_metadata.json\")\n",
    "\n",
    "    # 3) Models roots + json_stem\n",
    "    for root in models_roots:\n",
    "        cand_paths.append(root / json_stem / \"training_metadata.json\")\n",
    "\n",
    "    # First pass: any candidate that exists\n",
    "    for c in cand_paths:\n",
    "        if c.exists():\n",
    "            vr = _load_training_vram_from_path(c)\n",
    "            if vr:\n",
    "                if DEBUG:\n",
    "                    print(f\"[DEBUG] Using training metadata (direct): {c}\")\n",
    "                return vr\n",
    "\n",
    "    # 4) Deep search under each Models root\n",
    "    for root in models_roots:\n",
    "        try:\n",
    "            hits = list(root.rglob(\"training_metadata.json\"))\n",
    "        except Exception:\n",
    "            hits = []\n",
    "        # Prefer parent name match\n",
    "        for h in hits:\n",
    "            if h.parent.name in {json_stem, Path(model_name_in_json or '').name}:\n",
    "                vr = _load_training_vram_from_path(h)\n",
    "                if vr:\n",
    "                    if DEBUG:\n",
    "                        print(f\"[DEBUG] Using training metadata (parent match): {h}\")\n",
    "                    return vr\n",
    "        # Try matching model_info.model_name inside file\n",
    "        for h in hits:\n",
    "            try:\n",
    "                with open(h, \"r\", encoding=\"utf-8\") as f:\n",
    "                    tm = json.load(f)\n",
    "                mi = (tm.get(\"model_info\") or {}).get(\"model_name\")\n",
    "                if mi and mi in {json_stem, _safe_name(json_stem)}:\n",
    "                    vr = _load_training_vram_from_path(h)\n",
    "                    if vr:\n",
    "                        if DEBUG:\n",
    "                            print(f\"[DEBUG] Using training metadata (model_info match): {h}\")\n",
    "                        return vr\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    if DEBUG:\n",
    "        print(f\"[DEBUG] No training metadata found for {json_stem}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# VRAM columns we always want visible\n",
    "VRAM_COLS = [\n",
    "    \"train_peak_vram_reserved_gb\",\n",
    "    \"train_peak_vram_allocated_gb\",\n",
    "    \"eval_peak_vram_reserved_gb\",\n",
    "    \"eval_peak_vram_allocated_gb\",\n",
    "]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "rows: List[dict] = []\n",
    "vram_cols_order: list[str] = VRAM_COLS.copy()\n",
    "col_order: list[str] = []\n",
    "\n",
    "eval_jsons = _gather_eval_jsons(METRIC_DIRS)\n",
    "if not eval_jsons and DEBUG:\n",
    "    print(\"[DEBUG] No evaluation JSONs found. Check METRIC_DIRS or where your evaluator saves files.\")\n",
    "\n",
    "for jf in eval_jsons:\n",
    "    try:\n",
    "        with open(jf, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception:\n",
    "        if DEBUG:\n",
    "            print(f\"[DEBUG] Skipping unreadable JSON: {jf}\")\n",
    "        continue\n",
    "\n",
    "    model_id = jf.stem  # metrics filename without extension\n",
    "    row = {\"model\": model_id}\n",
    "\n",
    "    # ---- EVAL VRAM ----\n",
    "    hw = data.get(\"hardware\") or {}\n",
    "    summ = data.get(\"summary\") or {}\n",
    "    row[\"eval_peak_vram_reserved_gb\"]  = hw.get(\"peak_vram_reserved_gb\",  summ.get(\"peak_vram_reserved_gb\"))\n",
    "    row[\"eval_peak_vram_allocated_gb\"] = hw.get(\"peak_vram_allocated_gb\", summ.get(\"peak_vram_allocated_gb\"))\n",
    "\n",
    "    # ---- TRAIN VRAM ----\n",
    "    vr = _find_training_metadata(data.get(\"model_name\"), model_id)\n",
    "    if vr:\n",
    "        row[\"train_peak_vram_reserved_gb\"], row[\"train_peak_vram_allocated_gb\"] = vr\n",
    "    else:\n",
    "        row[\"train_peak_vram_reserved_gb\"] = None\n",
    "        row[\"train_peak_vram_allocated_gb\"] = None\n",
    "\n",
    "    # ---- Per-dataset metrics ----\n",
    "    datasets = data.get(\"datasets\", {}) or {}\n",
    "    for ds_name, ds_block in datasets.items():\n",
    "        slug = dataset_slug(ds_name)\n",
    "        ds_type = (ds_block.get(\"type\") or \"\").lower()\n",
    "\n",
    "        if ds_type.startswith(\"bool\") or slug.startswith(\"bool\"):\n",
    "            continue\n",
    "\n",
    "        eff = choose_efficacy_metric(ds_block)\n",
    "        metric_col = None\n",
    "        if eff is not None:\n",
    "            metric_name, value = eff\n",
    "            metric_col = f\"{slug}__{metric_name}\"\n",
    "            row[metric_col] = value\n",
    "\n",
    "        lat_mean = (ds_block.get(\"latency_seconds\") or {}).get(\"mean\")\n",
    "        latency_col = f\"{slug}__latency_mean_s\"\n",
    "        row[latency_col] = lat_mean\n",
    "\n",
    "        if metric_col is not None and metric_col not in col_order:\n",
    "            col_order.append(metric_col)\n",
    "        if latency_col not in col_order:\n",
    "            col_order.append(latency_col)\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "# =========================\n",
    "# Build & save\n",
    "# =========================\n",
    "if rows:\n",
    "    df = pd.DataFrame(rows).set_index(\"model\")\n",
    "\n",
    "    # Ensure VRAM cols exist even if empty\n",
    "    for vc in VRAM_COLS:\n",
    "        if vc not in df.columns:\n",
    "            df[vc] = pd.NA\n",
    "\n",
    "    # Column order: VRAM first, then dataset metrics\n",
    "    final_cols = [c for c in VRAM_COLS if c in df.columns]\n",
    "    final_cols += [c for c in col_order if c in df.columns and c not in final_cols]\n",
    "    df = df.reindex(columns=final_cols)\n",
    "\n",
    "    # Sorting (same as before)\n",
    "    info_df = df.index.to_series().apply(parse_model_info).apply(pd.Series)\n",
    "    info_df.index.name = \"model\"\n",
    "    df = df.join(info_df)\n",
    "    df = df.sort_values(\n",
    "        by=[\"dataset_order\", \"dataset\", \"lora_group\", \"lora_rank\", \"size_num\", \"model\"],\n",
    "        ascending=[True,           True,       True,        False,       True,      True],\n",
    "    ).drop(columns=[\"dataset_order\", \"lora_group\", \"lora_rank\", \"is_nopeft\", \"is_base\", \"size_num\", \"dataset\"], errors=\"ignore\")\n",
    "else:\n",
    "    df = pd.DataFrame(columns=[\"model\"] + VRAM_COLS).set_index(\"model\")\n",
    "\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(OUT_CSV, index=True)\n",
    "\n",
    "print(f\"âœ… Wrote summary with shape {df.shape} to: {OUT_CSV}\")\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
