{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf77b4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote summary with shape (4, 9) to: metrics_summary.csv\n",
      "                          ai2_arc__accuracy  ai2_arc__latency_mean_s  ai2_arc__macro_f1  boolq__MCC  boolq__accuracy  boolq__latency_mean_s  squad_v2__EM  squad_v2__F1  squad_v2__latency_mean_s\n",
      "model                                                                                                                                                                                            \n",
      "Qwen3-0.6B-arc_SFT_QLORA              43.33                   1.0665             0.4184      0.0689             55.5                 0.3276           0.0          1.07                    0.3735\n",
      "Qwen3-0.6B_base                       56.67                   1.8267             0.5833      0.0716             46.0                 0.1894           7.0          9.13                    0.2485\n",
      "Qwen3-1.7B-arc_SFT_QLORA              76.67                  11.4777             0.7667      0.3421             62.5                 0.3900          18.0         23.31                    0.5195\n",
      "Qwen3-1.7B_base                       76.67                   4.1687             0.8031     -0.9797              1.0                 0.2246          28.0         32.11                    0.3137\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Config ----\n",
    "metrics_dir = Path(\"metrics\")\n",
    "out_csv = Path(\"metrics_summary.csv\")\n",
    "\n",
    "def dataset_slug(name: str) -> str:\n",
    "    # \"test-ai2_arc.parquet\" -> \"ai2_arc\"\n",
    "    n = name\n",
    "    if n.endswith(\".parquet\"):\n",
    "        n = n[:-8]\n",
    "    if n.startswith(\"test-\"):\n",
    "        n = n[5:]\n",
    "    return n\n",
    "\n",
    "def pick_two_metrics(ds_block: dict) -> list[tuple[str, float]]:\n",
    "    \"\"\"Return list of (metric_name, value) for the primary two metrics of this dataset.\"\"\"\n",
    "    kind = ds_block.get(\"type\", \"\")\n",
    "    m = ds_block.get(\"metrics\", {}) or {}\n",
    "\n",
    "    # Priority by known kinds\n",
    "    if kind == \"mcq4\":\n",
    "        candidates = [(\"accuracy\", ds_block.get(\"accuracy\")), (\"macro_f1\", m.get(\"macro_f1\"))]\n",
    "    elif kind == \"boolq\":\n",
    "        candidates = [(\"accuracy\", ds_block.get(\"accuracy\")), (\"MCC\", m.get(\"MCC\"))]\n",
    "    elif kind == \"squad_v2\":\n",
    "        candidates = [(\"EM\", m.get(\"EM\")), (\"F1\", m.get(\"F1\"))]\n",
    "    else:\n",
    "        # Fallback: take first two available among common keys\n",
    "        pool = [\n",
    "            (\"accuracy\", ds_block.get(\"accuracy\")),\n",
    "            (\"macro_f1\", m.get(\"macro_f1\")),\n",
    "            (\"F1\", m.get(\"F1\")),\n",
    "            (\"balanced_accuracy\", m.get(\"balanced_accuracy\")),\n",
    "            (\"MCC\", m.get(\"MCC\")),\n",
    "        ]\n",
    "        candidates = [p for p in pool if p[1] is not None]\n",
    "\n",
    "    # Keep only the first two non-None\n",
    "    chosen = []\n",
    "    for name, val in candidates:\n",
    "        if val is not None:\n",
    "            chosen.append((name, val))\n",
    "        if len(chosen) == 2:\n",
    "            break\n",
    "    return chosen\n",
    "\n",
    "rows = []\n",
    "\n",
    "for jf in sorted(metrics_dir.glob(\"*.json\")):\n",
    "    with open(jf, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Use file name (without extension) as model id; matches how you save metrics JSONs\n",
    "    model_id = jf.stem\n",
    "\n",
    "    row = {\"model\": model_id}\n",
    "    datasets = data.get(\"datasets\", {}) or {}\n",
    "    for ds_name, ds_block in datasets.items():\n",
    "        slug = dataset_slug(ds_name)\n",
    "\n",
    "        # main 2 metrics\n",
    "        for metric_name, value in pick_two_metrics(ds_block):\n",
    "            row[f\"{slug}__{metric_name}\"] = value\n",
    "\n",
    "        # add latency mean in seconds (per dataset)\n",
    "        lat_mean = (ds_block.get(\"latency_seconds\") or {}).get(\"mean\")\n",
    "        row[f\"{slug}__latency_mean_s\"] = lat_mean\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "# Build DataFrame\n",
    "if rows:\n",
    "    df = pd.DataFrame(rows).set_index(\"model\").sort_index(axis=1)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=[\"model\"]).set_index(\"model\")\n",
    "\n",
    "# Save CSV\n",
    "out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(out_csv, index=True)\n",
    "\n",
    "print(f\"✅ Wrote summary with shape {df.shape} to: {out_csv}\")\n",
    "with pd.option_context(\"display.max_columns\", None, \"display.width\", 200):\n",
    "    print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
