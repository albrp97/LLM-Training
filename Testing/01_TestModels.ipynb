{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "128ce3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import string\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from transformers.utils.logging import set_verbosity_error\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import statistics as stats\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Union, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f706a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map = {\"\": 0} if torch.cuda.is_available() else {\"\": \"cpu\"}\n",
    "# Define custom load function\n",
    "def load_custom_model(model_dir):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=device_map)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Define chat function\n",
    "def chat(model, tokenizer, user_prompt, system_prompt, max_new_tokens=1000):\n",
    "    messages = []\n",
    "    \n",
    "    messages.append({'role': 'user', 'content': user_prompt})\n",
    "    messages.append({'role': 'system', 'content': system_prompt})\n",
    "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", truncation=True, max_length=model.config.max_position_embeddings, enable_thinking=False).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs, max_new_tokens=max_new_tokens, do_sample=False, top_k=None, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, temperature=None, top_p=None)\n",
    "\n",
    "    model_response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "    \n",
    "\n",
    "    return model_response\n",
    "\n",
    "def evaluate_answer(model_output: str, correct_answer: str) -> bool:\n",
    "    match = re.search(r\"\\\\boxed\\{(.+?)\\}\", model_output)\n",
    "    if not match:\n",
    "        return False  # No valid boxed answer found\n",
    "\n",
    "    extracted = match.group(1).upper()\n",
    "    is_correct = extracted == correct_answer.upper()\n",
    "\n",
    "    return is_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_info = {\n",
    "    \"test-ai2_arc.parquet\": {\n",
    "        \"system_prompt\": (\n",
    "            \"You are taking a multiple-choice test.\\n\"\n",
    "            \"Each question will have exactly 4 options: A, B, C or D.\\n\"\n",
    "            \"Read the question and choose the correct answer.\\n\"\n",
    "            \"Output only the letter of the correct answer inside \\\\boxed{}, like this: \\\\boxed{C}\"\n",
    "        ),\n",
    "        \"context\": False,\n",
    "        \"task\": \"mcq4\",\n",
    "    },\n",
    "    \"test-boolq.parquet\": {\n",
    "        \"system_prompt\": (\n",
    "            \"You are answering a True/False question.\\n\"\n",
    "            \"The question will be accompanied by a short passage of context.\\n\"\n",
    "            \"Your answer must be either False or True.\\n\"\n",
    "            \"Output your answer inside \\\\boxed{}, like this: \\\\boxed{True}\"\n",
    "        ),\n",
    "        \"context\": True,\n",
    "        \"task\": \"boolq\",\n",
    "    },\n",
    "    \"test-squad_v2.parquet\": {\n",
    "        \"system_prompt\": (\n",
    "            \"You are answering a question based on a passage.\\n\"\n",
    "            \"Read the context carefully and provide the exact answer span from the passage.\\n\"\n",
    "            \"Do not add extra words or explanations.\\n\"\n",
    "            \"Output your answer inside \\\\boxed{}, like this: \\\\boxed{Einstein}\"\n",
    "        ),\n",
    "        \"context\": True,\n",
    "        \"task\": \"squad_v2\",\n",
    "    },\n",
    "    \"test-OpenMathInstruct-2.parquet\": {\n",
    "        \"system_prompt\": (\n",
    "            \"You are solving math problems.\\n\"\n",
    "            \"Read each problem carefully and provide only the final numeric answer.\\n\"\n",
    "            \"Output your answer inside \\\\boxed{}, like this: \\\\boxed{42}\\n\"\n",
    "            \"Do not include any explanation or intermediate steps.\"\n",
    "        ),\n",
    "        \"context\": False,\n",
    "        \"task\": \"math_numeric\",\n",
    "    }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c41a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS\n",
    "# ---------------------------\n",
    "# Parsing + normalization helpers\n",
    "# ---------------------------\n",
    "_BOXED_RE = re.compile(r\"\\\\boxed\\{\\s*(.*?)\\s*\\}\", flags=re.DOTALL)\n",
    "def extract_boxed(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    m = _BOXED_RE.search(text)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "def normalize_arc_option(s: str):\n",
    "    if s is None:\n",
    "        return None\n",
    "    t = s.strip().upper()\n",
    "    if len(t) > 1:\n",
    "        for ch in t:\n",
    "            if ch in {\"A\", \"B\", \"C\", \"D\"}:\n",
    "                return ch\n",
    "        return None\n",
    "    return t if t in {\"A\", \"B\", \"C\", \"D\"} else None\n",
    "\n",
    "def normalize_bool(s: str):\n",
    "    if s is None: return None\n",
    "    t = s.strip().lower()\n",
    "    if t in {\"true\",\"t\",\"yes\",\"y\",\"1\"}: return True\n",
    "    if t in {\"false\",\"f\",\"no\",\"n\",\"0\"}: return False\n",
    "    return None\n",
    "\n",
    "_ARTICLES = {\"a\", \"an\", \"the\"}\n",
    "_PUNCT_TABLE = str.maketrans(\"\", \"\", string.punctuation)\n",
    "_EMPTY_ANSWERS = {\"\", \"unanswerable\", \"unknown\", \"no answer\", \"n a\", \"none\", \"null\"}\n",
    "\n",
    "def squad_normalize(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = s.lower()\n",
    "    s = s.translate(_PUNCT_TABLE)\n",
    "    tokens = [w for w in s.split() if w not in _ARTICLES]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def squad_em(pred: str, golds):\n",
    "    pn = squad_normalize(pred)\n",
    "    for g in (golds if isinstance(golds, list) else [golds]):\n",
    "        if pn == squad_normalize(str(g)):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def squad_f1(pred: str, golds):\n",
    "    pn = squad_normalize(pred).split()\n",
    "    best = 0.0\n",
    "    for g in (golds if isinstance(golds, list) else [golds]):\n",
    "        gn = squad_normalize(str(g)).split()\n",
    "        if not pn and not gn:\n",
    "            best = max(best, 1.0); continue\n",
    "        if not pn or not gn:\n",
    "            best = max(best, 0.0); continue\n",
    "        common = Counter(pn) & Counter(gn)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            best = max(best, 0.0); continue\n",
    "        precision = num_same / len(pn)\n",
    "        recall = num_same / len(gn)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        best = max(best, f1)\n",
    "    return best\n",
    "\n",
    "def is_empty_like(s: str) -> bool:\n",
    "    return squad_normalize(s) in _EMPTY_ANSWERS\n",
    "\n",
    "# ---------------------------\n",
    "# Unified metrics dispatcher\n",
    "# ---------------------------\n",
    "def compute_metrics(kind: str, golds, preds):\n",
    "    \"\"\"\n",
    "    kind: \"mcq4\" | \"boolq\" | \"squad_v2\" | \"generic\"\n",
    "    golds/preds: lists aligned with dataset kind\n",
    "    Returns: {\n",
    "        correct, incorrect, accuracy,\n",
    "        metrics: {...}   # task-specific fields\n",
    "    }\n",
    "    \"\"\"\n",
    "    if kind == \"mcq4\":\n",
    "        classes = [\"A\",\"B\",\"C\",\"D\"]; idx = {c:i for i,c in enumerate(classes)}\n",
    "        cm = [[0]*4 for _ in range(4)]\n",
    "        correct = total = 0\n",
    "        for t, p in zip(golds, preds):\n",
    "            total += 1\n",
    "            if p is not None and t == p:\n",
    "                correct += 1\n",
    "            if (t in idx) and (p in idx):\n",
    "                cm[idx[t]][idx[p]] += 1\n",
    "        per_class = {}\n",
    "        f1s = []\n",
    "        for i,c in enumerate(classes):\n",
    "            tp = cm[i][i]\n",
    "            fp = sum(cm[r][i] for r in range(4)) - tp\n",
    "            fn = sum(cm[i][r] for r in range(4)) - tp\n",
    "            prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "            rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "            f1   = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "            per_class[c] = {\"precision\": round(prec,4), \"recall\": round(rec,4), \"f1\": round(f1,4)}\n",
    "            f1s.append(f1)\n",
    "        acc = (correct/total*100) if total else 0.0\n",
    "        macro_f1 = sum(f1s)/len(f1s) if f1s else 0.0\n",
    "        return {\n",
    "            \"correct\": correct,\n",
    "            \"incorrect\": total - correct,\n",
    "            \"accuracy\": round(acc, 2),\n",
    "            \"metrics\": {\n",
    "                \"task\": \"ARC (4-way MCQ)\",\n",
    "                \"macro_f1\": round(macro_f1,4),\n",
    "                \"per_class\": per_class,\n",
    "                \"confusion_matrix\": {classes[i]: {classes[j]: cm[i][j] for j in range(4)} for i in range(4)},\n",
    "                \"support\": total,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    if kind == \"boolq\":\n",
    "        TP=TN=FP=FN=0\n",
    "        for t,p in zip(golds, preds):\n",
    "            if p is None:\n",
    "                if t: FN += 1\n",
    "                else: FP += 1\n",
    "            elif t and p: TP += 1\n",
    "            elif (not t) and (not p): TN += 1\n",
    "            elif (not t) and p: FP += 1\n",
    "            elif t and (not p): FN += 1\n",
    "        total = TP+TN+FP+FN\n",
    "        acc = (TP+TN)/total*100 if total else 0.0\n",
    "        prec = TP/(TP+FP) if (TP+FP)>0 else 0.0\n",
    "        rec  = TP/(TP+FN) if (TP+FN)>0 else 0.0\n",
    "        f1   = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "        prec_n = TN/(TN+FN) if (TN+FN)>0 else 0.0\n",
    "        rec_n  = TN/(TN+FP) if (TN+FP)>0 else 0.0\n",
    "        f1_n   = 2*prec_n*rec_n/(prec_n+rec_n) if (prec_n+rec_n)>0 else 0.0\n",
    "        macro_f1 = (f1 + f1_n)/2\n",
    "        tpr = rec\n",
    "        tnr = TN/(TN+FP) if (TN+FP)>0 else 0.0\n",
    "        bal_acc = (tpr+tnr)/2\n",
    "        denom = math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "        mcc = ((TP*TN - FP*FN)/denom) if denom>0 else 0.0\n",
    "        return {\n",
    "            \"correct\": TP+TN,\n",
    "            \"incorrect\": FP+FN,\n",
    "            \"accuracy\": round(acc, 2),\n",
    "            \"metrics\": {\n",
    "                \"task\": \"BoolQ (binary)\",\n",
    "                \"f1_pos\": round(f1,4),\n",
    "                \"macro_f1\": round(macro_f1,4),\n",
    "                \"balanced_accuracy\": round(bal_acc,4),\n",
    "                \"MCC\": round(mcc,4),\n",
    "                \"confusion_matrix\": {\"TP\":TP,\"TN\":TN,\"FP\":FP,\"FN\":FN},\n",
    "                \"support\": total,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    if kind == \"squad_v2\":\n",
    "        ems, f1s, has_g, has_p = [], [], [], []\n",
    "        for gold, pred in zip(golds, preds):\n",
    "            ems.append(squad_em(pred, gold))\n",
    "            f1s.append(squad_f1(pred, gold))\n",
    "            g_has = not all(is_empty_like(str(g)) for g in (gold if isinstance(gold, list) else [gold]))\n",
    "            p_has = not is_empty_like(pred)\n",
    "            has_g.append(g_has); has_p.append(p_has)\n",
    "        total = len(golds)\n",
    "        def _avg(x): return sum(x)/len(x) if x else 0.0\n",
    "        EM = _avg(ems)*100; F1 = _avg(f1s)*100\n",
    "        has_idx = [i for i,g in enumerate(has_g) if g]\n",
    "        no_idx  = [i for i,g in enumerate(has_g) if not g]\n",
    "        HasAns_EM = _avg([ems[i] for i in has_idx])*100 if has_idx else 0.0\n",
    "        HasAns_F1 = _avg([f1s[i] for i in has_idx])*100 if has_idx else 0.0\n",
    "        NoAns_Acc = _avg([1.0 if not has_p[i] else 0.0 for i in no_idx])*100 if no_idx else 0.0\n",
    "        # AvNA (answer vs no-answer)\n",
    "        TP=TN=FP=FN=0\n",
    "        for g,p in zip(has_g,has_p):\n",
    "            if g and p: TP+=1\n",
    "            elif (not g) and (not p): TN+=1\n",
    "            elif (not g) and p: FP+=1\n",
    "            elif g and (not p): FN+=1\n",
    "        AvNA = (TP+TN)/total*100 if total else 0.0\n",
    "        # define \"correct\" as exact matches\n",
    "        correct = int(sum(ems))\n",
    "        return {\n",
    "            \"correct\": correct,\n",
    "            \"incorrect\": total - correct,\n",
    "            \"accuracy\": round(EM, 2),  # EM as accuracy\n",
    "            \"metrics\": {\n",
    "                \"task\": \"SQuAD v2\",\n",
    "                \"EM\": round(EM,2),\n",
    "                \"F1\": round(F1,2),\n",
    "                \"HasAns_EM\": round(HasAns_EM,2),\n",
    "                \"HasAns_F1\": round(HasAns_F1,2),\n",
    "                \"NoAns_Accuracy\": round(NoAns_Acc,2),\n",
    "                \"AvNA_Accuracy\": round(AvNA,2),\n",
    "                \"support\": total,\n",
    "            }\n",
    "        }\n",
    "    if kind == \"math_numeric\":\n",
    "        # Comparar respuestas numéricas con tolerancia\n",
    "        correct = 0\n",
    "        total = len(golds)\n",
    "        incorrect = 0\n",
    "        tolerance = 1e-4  # puedes ajustar la tolerancia si lo deseas\n",
    "        diffs = []\n",
    "        for gold, pred in zip(golds, preds):\n",
    "            try:\n",
    "                gold_num = float(gold)\n",
    "                pred_num = float(pred) if pred is not None and pred != \"\" else None\n",
    "                if pred_num is not None and abs(gold_num - pred_num) < tolerance:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    incorrect += 1\n",
    "                if pred_num is not None:\n",
    "                    diffs.append(abs(gold_num - pred_num))\n",
    "            except Exception:\n",
    "                incorrect += 1\n",
    "                diffs.append(None)\n",
    "        acc = correct / total * 100 if total else 0.0\n",
    "        avg_diff = sum([d for d in diffs if d is not None]) / len([d for d in diffs if d is not None]) if diffs else None\n",
    "        return {\n",
    "            \"correct\": correct,\n",
    "            \"incorrect\": incorrect,\n",
    "            \"accuracy\": round(acc, 2),\n",
    "            \"metrics\": {\n",
    "                \"task\": \"OpenMathInstruct-2 (numeric)\",\n",
    "                \"avg_abs_diff\": round(avg_diff, 6) if avg_diff is not None else None,\n",
    "                \"support\": total,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # generic fallback: string equality\n",
    "    correct = sum(1 for g,p in zip(golds,preds) if (p is not None and p == g))\n",
    "    total = len(golds)\n",
    "    acc = correct/total*100 if total else 0.0\n",
    "    return {\n",
    "        \"correct\": correct,\n",
    "        \"incorrect\": total - correct,\n",
    "        \"accuracy\": round(acc, 2),\n",
    "        \"metrics\": {\"task\": \"Generic\", \"support\": total}\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e42309d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VRAM Report: Before inference @ 14:06:36 ===\n",
      "[GPU 0] NVIDIA GeForce RTX 4090\n",
      "  Total:  23.99 GB\n",
      "  Used*:  1.53 GB   (*overall, all processes)\n",
      "  Free:   22.45 GB\n",
      "  PyTorch Allocated: 0.00 B   (your tensors)\n",
      "  PyTorch Reserved:  0.00 B (cache for reuse)\n",
      "  Peak Allocated:    0.00 B\n",
      "  Peak Reserved:     0.00 B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, gc\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "def _fmt_bytes(n: int) -> str:\n",
    "    for unit in (\"B\",\"KB\",\"MB\",\"GB\",\"TB\"):\n",
    "        if n < 1024 or unit == \"TB\":\n",
    "            return f\"{n:.2f} {unit}\"\n",
    "        n /= 1024\n",
    "\n",
    "def print_vram_report(title: str = \"\"):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available.\")\n",
    "        return\n",
    "    if title:\n",
    "        print(f\"\\n=== VRAM Report: {title} @ {datetime.now().strftime('%H:%M:%S')} ===\")\n",
    "    else:\n",
    "        print(f\"\\n=== VRAM Report @ {datetime.now().strftime('%H:%M:%S')} ===\")\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        name = torch.cuda.get_device_name(i)\n",
    "        free, total = torch.cuda.mem_get_info(i)  # bytes\n",
    "        used_total = total - free                 # everything used on the GPU\n",
    "        alloc = torch.cuda.memory_allocated(i)    # tensors by *this* process\n",
    "        reserved = torch.cuda.memory_reserved(i)  # cached by PyTorch for reuse\n",
    "\n",
    "        print(f\"[GPU {i}] {name}\")\n",
    "        print(f\"  Total:  {_fmt_bytes(total)}\")\n",
    "        print(f\"  Used*:  {_fmt_bytes(used_total)}   (*overall, all processes)\")\n",
    "        print(f\"  Free:   {_fmt_bytes(free)}\")\n",
    "        print(f\"  PyTorch Allocated: {_fmt_bytes(alloc)}   (your tensors)\")\n",
    "        print(f\"  PyTorch Reserved:  {_fmt_bytes(reserved)} (cache for reuse)\")\n",
    "        # Peak stats since last reset (see helpers below)\n",
    "        try:\n",
    "            peak_alloc = torch.cuda.max_memory_allocated(i)\n",
    "            peak_reserved = torch.cuda.max_memory_reserved(i)\n",
    "            print(f\"  Peak Allocated:    {_fmt_bytes(peak_alloc)}\")\n",
    "            print(f\"  Peak Reserved:     {_fmt_bytes(peak_reserved)}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    print()\n",
    "\n",
    "def reset_vram():\n",
    "    gc.collect()\n",
    "\n",
    "    # actually release CUDA caches on every GPU\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            with torch.cuda.device(i):\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.ipc_collect()\n",
    "\n",
    "reset_vram()\n",
    "print_vram_report(\"Before inference\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e99b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTrunc=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e13948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..\\\\Models\\\\Qwen3-0.6B-arc_SFT_QLORA', '..\\\\Models\\\\Qwen3-0.6B_base', '..\\\\Models\\\\Qwen3-1.7B-arc_SFT_QLORA', '..\\\\Models\\\\Qwen3-1.7B_base']\n",
      "\n",
      "=== VRAM Report: model_name @ 14:06:36 ===\n",
      "[GPU 0] NVIDIA GeForce RTX 4090\n",
      "  Total:  23.99 GB\n",
      "  Used*:  1.53 GB   (*overall, all processes)\n",
      "  Free:   22.45 GB\n",
      "  PyTorch Allocated: 0.00 B   (your tensors)\n",
      "  PyTorch Reserved:  0.00 B (cache for reuse)\n",
      "  Peak Allocated:    0.00 B\n",
      "  Peak Reserved:     0.00 B\n",
      "\n",
      "------------\n",
      "\n",
      "..\\Models\\Qwen3-0.6B-arc_SFT_QLORA\n"
     ]
    }
   ],
   "source": [
    "# TODO check if model and dataset already processed and skip if so\n",
    "\n",
    "# Directory with Fine Tuned models\n",
    "models_dir = Path(\"../Models\")\n",
    "\n",
    "models = []\n",
    "\n",
    "for model_dir in models_dir.iterdir():\n",
    "    if not model_dir.is_dir():\n",
    "        continue\n",
    "    \n",
    "    metadata_path = model_dir / \"training_metadata.json\"\n",
    "    if not metadata_path.exists():\n",
    "        continue\n",
    "    \n",
    "    # Leer metadata\n",
    "    with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    base_model_name = metadata[\"model_info\"][\"base_model\"]\n",
    "    \n",
    "    models.append(str(model_dir))\n",
    "\n",
    "\n",
    "datasets = list(Path(\"../Datasets\").glob('test-*.parquet'))\n",
    "\n",
    "metrics_dir = Path(\"metrics\")\n",
    "metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results_file = Path(\"benchmark_results.json\")\n",
    "if results_file.exists():\n",
    "    with open(results_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        results = json.load(f)\n",
    "else:\n",
    "    results = {}\n",
    "\n",
    "print(models)\n",
    "\n",
    "\n",
    "VERBOSE = False  # set to False to hide per-dataset logs and summary prints\n",
    "\n",
    "def vlog(msg: str):\n",
    "    if VERBOSE:\n",
    "        tqdm.write(msg)\n",
    "\n",
    "def _safe_filename(name: str) -> str:\n",
    "    # If it's a HuggingFace repo id like \"org/model\" (optionally with a revision \"@rev\"),\n",
    "    # keep both parts but replace \"/\" with \"__\".\n",
    "    if re.match(r'^[\\w\\-]+/[\\w\\.\\-]+(@[\\w\\.\\-]+)?$', name):\n",
    "        base = name.replace('/', '__')\n",
    "    else:\n",
    "        # Treat as local path → keep only the last component (folder/file name)\n",
    "        base = Path(name).name\n",
    "\n",
    "    # Sanitize anything that Windows wouldn't like or that could create dirs\n",
    "    base = re.sub(r'[<>:\"/\\\\|?*\\x00-\\x1F]', '_', base)\n",
    "    return base\n",
    "\n",
    "def _mode(values):\n",
    "    if not values:\n",
    "        return None\n",
    "    try:\n",
    "        return stats.mode(values)  # unique mode\n",
    "    except stats.StatisticsError:\n",
    "        mm = stats.multimode(values)\n",
    "        return mm[0] if mm else None\n",
    "\n",
    "for model_name in models:\n",
    "    reset_vram()\n",
    "    print_vram_report(\"model_name\")\n",
    "    print('------------\\n')\n",
    "    print(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=device_map)\n",
    "\n",
    "    # preload\n",
    "    # if truncation is none then we load the entire dataset\n",
    "    if datasetTrunc is None:\n",
    "        loaded = [(ds, pd.read_parquet(ds)) for ds in datasets]\n",
    "    else:\n",
    "        loaded = [(ds, pd.read_parquet(ds).head(datasetTrunc)) for ds in datasets]\n",
    "    total_samples = sum(len(df) for _, df in loaded)\n",
    "\n",
    "    per_dataset = {}\n",
    "    all_latencies, all_tokens = [], []\n",
    "\n",
    "    if VERBOSE:\n",
    "        tqdm.write(f\"Model: {model_name}\")\n",
    "\n",
    "    with tqdm(\n",
    "        total=total_samples,\n",
    "        desc=model_name,\n",
    "        dynamic_ncols=True,\n",
    "        bar_format=\"Testing with {desc} ({n_fmt}/{total_fmt}) |{bar}| {percentage:3.0f}% {rate_fmt} {elapsed}<{remaining}\",\n",
    "        leave=True\n",
    "    ) as pbar:\n",
    "\n",
    "        for dataset_path, df in loaded:\n",
    "            dataset_name = dataset_path.name\n",
    "            info = datasets_info.get(dataset_name, {\"system_prompt\":\"\", \"context\": False, \"task\":\"generic\"})\n",
    "            kind = info.get(\"task\", \"generic\")\n",
    "\n",
    "            latencies, tokens_out = [], []\n",
    "            format_ok = 0\n",
    "\n",
    "            # golds/preds for dispatcher\n",
    "            golds, preds = [], []\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                user_prompt = row[\"question\"]\n",
    "                if info[\"context\"]:\n",
    "                    user_prompt += row[\"context\"]\n",
    "\n",
    "                t0 = time.perf_counter()\n",
    "                response = chat(model, tokenizer, user_prompt, info[\"system_prompt\"])\n",
    "                dt = time.perf_counter() - t0\n",
    "                latencies.append(dt)\n",
    "\n",
    "                try:\n",
    "                    tok_count = len(tokenizer.encode(response, add_special_tokens=False))\n",
    "                except Exception:\n",
    "                    tok_count = 0\n",
    "                tokens_out.append(tok_count)\n",
    "\n",
    "                pred_boxed = extract_boxed(response)\n",
    "                if pred_boxed is not None:\n",
    "                    format_ok += 1\n",
    "\n",
    "                # collect gold/pred by kind\n",
    "                if kind == \"mcq4\":\n",
    "                    golds.append(normalize_arc_option(str(row[\"answer\"])))\n",
    "                    preds.append(normalize_arc_option(pred_boxed))\n",
    "                elif kind == \"boolq\":\n",
    "                    golds.append(True if str(row[\"answer\"]).strip().lower() == \"true\" else False)\n",
    "                    preds.append(normalize_bool(pred_boxed))\n",
    "                elif kind == \"squad_v2\":\n",
    "                    golds.append(row[\"answer\"])  # string or list of strings\n",
    "                    preds.append((pred_boxed or \"\").strip())\n",
    "                else:\n",
    "                    golds.append(str(row[\"answer\"]).strip())\n",
    "                    preds.append((pred_boxed or \"\").strip())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "            # latency/token aggregates\n",
    "            all_latencies.extend(latencies)\n",
    "            all_tokens.extend(tokens_out)\n",
    "\n",
    "            lat_sum = float(sum(latencies))\n",
    "            lat_mean = (lat_sum / len(latencies)) if latencies else 0.0\n",
    "            lat_median = stats.median(latencies) if latencies else 0.0\n",
    "            lat_mode = _mode([round(x, 3) for x in latencies])\n",
    "            lat_std = stats.stdev(latencies) if len(latencies) > 1 else 0.0\n",
    "\n",
    "            tok_sum = int(sum(tokens_out))\n",
    "            tok_mean = (tok_sum / len(tokens_out)) if tokens_out else 0.0\n",
    "            tok_median = stats.median(tokens_out) if tokens_out else 0.0\n",
    "            tok_mode = _mode(tokens_out)\n",
    "            tok_std = stats.stdev(tokens_out) if len(tokens_out) > 1 else 0.0\n",
    "\n",
    "            # compute dataset metrics via dispatcher\n",
    "            res = compute_metrics(kind, golds, preds)\n",
    "            total = len(golds)\n",
    "            format_rate = (format_ok / total * 100) if total else 0.0\n",
    "\n",
    "            per_dataset[dataset_name] = {\n",
    "                \"type\": kind,\n",
    "                \"total_samples\": total,\n",
    "                \"format_success_rate\": round(format_rate, 2),\n",
    "                \"correct\": int(res[\"correct\"]),\n",
    "                \"incorrect\": int(res[\"incorrect\"]),\n",
    "                \"accuracy\": round(res[\"accuracy\"], 2),\n",
    "                \"metrics\": res[\"metrics\"],\n",
    "                \"latency_seconds\": {\n",
    "                    \"per_prompt\": [round(x, 4) for x in latencies],\n",
    "                    \"sum\": round(lat_sum, 4),\n",
    "                    \"mean\": round(lat_mean, 4),\n",
    "                    \"median\": round(lat_median, 4),\n",
    "                    \"mode\": lat_mode,\n",
    "                    \"std\": round(lat_std, 4),\n",
    "                },\n",
    "                \"tokens_generated\": {\n",
    "                    \"per_prompt\": tokens_out,\n",
    "                    \"sum\": tok_sum,\n",
    "                    \"mean\": round(tok_mean, 2),\n",
    "                    \"median\": round(tok_median, 2),\n",
    "                    \"mode\": tok_mode,\n",
    "                    \"std\": round(tok_std, 2),\n",
    "                },\n",
    "            }\n",
    "\n",
    "            # optional logs\n",
    "            vlog(\"*\"*30)\n",
    "            vlog(f\"Dataset: {dataset_name} [{kind}]\")\n",
    "            if kind == \"mcq4\":\n",
    "                vlog(f\"🎯 Acc {res['accuracy']:.2f}% | Macro-F1 {res['metrics']['macro_f1']:.4f}\")\n",
    "            elif kind == \"boolq\":\n",
    "                vlog(f\"🎯 Acc {res['accuracy']:.2f}% | F1(pos) {res['metrics']['f1_pos']:.4f} | MCC {res['metrics']['MCC']:.4f}\")\n",
    "            elif kind == \"squad_v2\":\n",
    "                vlog(f\"🎯 EM {res['metrics']['EM']:.2f}% | F1 {res['metrics']['F1']:.2f}% | AvNA {res['metrics']['AvNA_Accuracy']:.2f}%\")\n",
    "            else:\n",
    "                vlog(f\"🎯 Acc {res['accuracy']:.2f}%\")\n",
    "            vlog(f\"⏱️ Latency sum/mean/median/mode/std -> {lat_sum:.3f} / {lat_mean:.3f} / {lat_median:.3f} / {lat_mode} / {lat_std:.3f}\")\n",
    "            vlog(f\"🔢 Tokens  sum/mean/median/mode/std -> {tok_sum} / {tok_mean:.2f} / {tok_median:.2f} / {tok_mode} / {tok_std:.2f}\")\n",
    "\n",
    "    # ----- model summary -----\n",
    "    grand_total = sum(v[\"total_samples\"] for v in per_dataset.values())\n",
    "    grand_correct = sum(v[\"correct\"] for v in per_dataset.values())\n",
    "    grand_incorrect = sum(v[\"incorrect\"] for v in per_dataset.values())\n",
    "    overall_acc = (grand_correct / grand_total * 100) if grand_total else 0.0\n",
    "\n",
    "    g_lat_sum = float(sum(all_latencies))\n",
    "    g_lat_mean = (g_lat_sum / len(all_latencies)) if all_latencies else 0.0\n",
    "    g_lat_median = stats.median(all_latencies) if all_latencies else 0.0\n",
    "    g_lat_mode = _mode([round(x, 3) for x in all_latencies]) if all_latencies else None\n",
    "    g_lat_std = stats.stdev(all_latencies) if len(all_latencies) > 1 else 0.0\n",
    "\n",
    "    g_tok_sum = int(sum(all_tokens))\n",
    "    g_tok_mean = (g_tok_sum / len(all_tokens)) if all_tokens else 0.0\n",
    "    g_tok_median = stats.median(all_tokens) if all_tokens else 0.0\n",
    "    g_tok_mode = _mode(all_tokens) if all_tokens else None\n",
    "    g_tok_std = stats.stdev(all_tokens) if len(all_tokens) > 1 else 0.0\n",
    "\n",
    "    model_report = {\n",
    "        \"model_name\": model_name,\n",
    "        \"evaluated_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"summary\": {\n",
    "            \"num_datasets\": len(per_dataset),\n",
    "            \"total_samples\": grand_total,\n",
    "            \"total_correct\": grand_correct,\n",
    "            \"total_incorrect\": grand_incorrect,\n",
    "            \"overall_accuracy\": round(overall_acc, 2),\n",
    "            \"latency_seconds\": {\n",
    "                \"sum\": round(g_lat_sum, 4),\n",
    "                \"mean\": round(g_lat_mean, 4),\n",
    "                \"median\": round(g_lat_median, 4),\n",
    "                \"mode\": g_lat_mode,\n",
    "                \"std\": round(g_lat_std, 4),\n",
    "            },\n",
    "            \"tokens_generated\": {\n",
    "                \"sum\": g_tok_sum,\n",
    "                \"mean\": round(g_tok_mean, 2),\n",
    "                \"median\": round(g_tok_median, 2),\n",
    "                \"mode\": g_tok_mode,\n",
    "                \"std\": round(g_tok_std, 2),\n",
    "            },\n",
    "        },\n",
    "        \"datasets\": per_dataset\n",
    "    }\n",
    "\n",
    "    out_path = metrics_dir / f\"{_safe_filename(model_name)}.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(model_report, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(f\"💾 Métricas guardadas en {out_path}\")\n",
    "    else:\n",
    "        print(f\"saved at {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8b078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VRAM Report: After inference @ 14:03:20 ===\n",
      "[GPU 0] NVIDIA GeForce RTX 4090\n",
      "  Total:  23.99 GB\n",
      "  Used*:  5.58 GB   (*overall, all processes)\n",
      "  Free:   18.41 GB\n",
      "  PyTorch Allocated: 1.30 GB   (your tensors)\n",
      "  PyTorch Reserved:  3.97 GB (cache for reuse)\n",
      "  Peak Allocated:    1.30 GB\n",
      "  Peak Reserved:     3.97 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_vram_report(\"After inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cf92c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VRAM Report: After clearing @ 14:03:23 ===\n",
      "[GPU 0] NVIDIA GeForce RTX 4090\n",
      "  Total:  23.99 GB\n",
      "  Used*:  5.58 GB   (*overall, all processes)\n",
      "  Free:   18.41 GB\n",
      "  PyTorch Allocated: 1.30 GB   (your tensors)\n",
      "  PyTorch Reserved:  3.97 GB (cache for reuse)\n",
      "  Peak Allocated:    1.30 GB\n",
      "  Peak Reserved:     3.97 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reset_vram()\n",
    "\n",
    "print_vram_report(\"After clearing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
