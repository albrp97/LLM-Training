{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f706a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/Documentos/Programacion/LLM-Training/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "device_map = {\"\": 0} if torch.cuda.is_available() else {\"\": \"cpu\"}\n",
    "# Define custom load function\n",
    "def load_custom_model(model_dir):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=device_map)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Define chat function\n",
    "def chat(model, tokenizer, user_prompt, system_prompt, max_new_tokens=1000):\n",
    "    messages = []\n",
    "    \n",
    "    messages.append({'role': 'user', 'content': user_prompt})\n",
    "    messages.append({'role': 'system', 'content': system_prompt})\n",
    "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", truncation=True, max_length=model.config.max_position_embeddings, enable_thinking=False).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs, max_new_tokens=max_new_tokens, do_sample=False, top_k=50, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    model_response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "    \n",
    "\n",
    "    return model_response\n",
    "\n",
    "def evaluate_answer(model_output: str, correct_answer: str) -> bool:\n",
    "    match = re.search(r\"\\\\boxed\\{(.+?)\\}\", model_output)\n",
    "    if not match:\n",
    "        return False  # No valid boxed answer found\n",
    "\n",
    "    extracted = match.group(1).upper()\n",
    "    is_correct = extracted == correct_answer.upper()\n",
    "\n",
    "    return is_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545b9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_info = {\n",
    "    \"test-ai2_arc.parquet\": {\n",
    "        \"system_prompt\": (\n",
    "            \"You are taking a multiple-choice test.\\n\"\n",
    "            \"Each question will have exactly 4 options: A, B, C or D.\\n\"\n",
    "            \"Read the question and choose the correct answer.\\n\"\n",
    "            \"Output the letter of the correct answer inside \\\\boxed{}, like this: \\\\boxed{C}\"\n",
    "        ),\n",
    "        \"context\": False,\n",
    "    },\n",
    "    \n",
    "    \"test-boolq.parquet\": {\n",
    "        \"system_prompt\": (\n",
    "            \"You are answering a True/False question.\\n\"\n",
    "            \"The question will be accompanied by a short passage of context.\\n\"\n",
    "            \"Your answer must be either False or True.\\n\"\n",
    "            \"Output your answer inside \\\\boxed{}, like this: \\\\boxed{True}\"\n",
    "        ),\n",
    "        \"context\": True,\n",
    "    },\n",
    "    \n",
    "    \"test-squad_v2.parquet\": {\n",
    "        \"system_prompt\": (\n",
    "            \"You are answering a question based on a passage.\\n\"\n",
    "            \"Read the context carefully and provide the exact answer span from the passage.\\n\"\n",
    "            \"Do not add extra words or explanations.\\n\"\n",
    "            \"Output your answer inside \\\\boxed{}, like this: \\\\boxed{Einstein}\"\n",
    "        ),\n",
    "        \"context\": True,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e13948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 31.79it/s]\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "Model: Qwen/Qwen3-1.7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "**********************\n",
      "Dataset: test-boolq.parquet\n",
      "âœ… Total Questions: 3\n",
      "âœ… Correct Answers: 0\n",
      "ðŸ“Š Accuracy: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France\n",
      "France\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10th and 11th centuries\n",
      "Normans in Normandy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denmark, Iceland and Norway\n",
      "Norse\n",
      "**********************\n",
      "Dataset: test-squad_v2.parquet\n",
      "âœ… Total Questions: 3\n",
      "âœ… Correct Answers: 0\n",
      "ðŸ“Š Accuracy: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "The question states that an astronomer observes a **planet rotating faster** after a **meteorite impact**. We are to determine the most likely effect of this increase in rotation.\n",
      "\n",
      "Let's analyze the options:\n",
      "\n",
      "- **A. Planetary density will decrease.**  \n",
      "  - Density is mass divided by volume. An increase in rotation does **not** directly affect density. Density depends on mass and volume, not rotation. So this is **not likely**.\n",
      "\n",
      "- **B. Planetary years will become longer.**  \n",
      "  - A planetary year is the time it takes to orbit the sun. Rotation and orbit are **distinct**. An increase in rotation does **not** affect the orbital period. So this is **not likely**.\n",
      "\n",
      "- **C. Planetary days will become shorter.**  \n",
      "  - A planetary day is the time it takes for a planet to rotate once on its axis. If the planet is rotating faster, its **day length** (rotational period) will **decrease**. This is **directly** related to the observed increase in rotation. So this is **likely**.\n",
      "\n",
      "- **D. Planetary gravity will become stronger.**  \n",
      "  - Gravity depends on mass and radius. An increase in rotation does **not** directly affect gravity. So this is **not likely**.\n",
      "\n",
      "### âœ… Correct Answer: \\boxed{C}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "The question asks what will most likely result from testing different building designs for their ability to withstand earthquakes.\n",
      "\n",
      "- **A. Buildings will be built faster** â€“ This is unlikely, as testing does not necessarily speed up construction.\n",
      "- **B. Buildings will be made safer** â€“ This is likely, as testing different designs to withstand earthquakes would aim to improve safety.\n",
      "- **C. Building designs will look nicer** â€“ This is not directly related to earthquake testing and is not a likely outcome.\n",
      "- **D. Building materials will be cheaper** â€“ This is not directly related to the testing process and is not a likely outcome.\n",
      "\n",
      "The most logical and direct result of testing different building designs for earthquake resistance is that **buildings will be made safer**.\n",
      "\n",
      "$$\n",
      "\\boxed{B}\n",
      "$$\n",
      "C\n",
      "The question asks which step signals the beginning of photosynthesis, and the end result is the production of sugar and oxygen.\n",
      "\n",
      "Photosynthesis begins with **light energy being absorbed** by chlorophyll in the leaf. This is the first step in the process, where light energy is used to convert carbon dioxide and water into glucose (a sugar) and oxygen.\n",
      "\n",
      "Let's analyze the options:\n",
      "\n",
      "- **A. Chemical energy is absorbed through the roots.**  \n",
      "  This is not correct. Photosynthesis does not involve chemical energy absorption through the roots.\n",
      "\n",
      "- **B. Light energy is converted to chemical energy.**  \n",
      "  This is part of the process, but it occurs **after** light energy is captured by chlorophyll.\n",
      "\n",
      "- **C. Chlorophyll in the leaf captures light energy.**  \n",
      "  This is the **beginning** of photosynthesis. It is the first step where light energy is absorbed and used to start the process.\n",
      "\n",
      "- **D. Sunlight is converted into chlorophyll.**  \n",
      "  This is incorrect. Chlorophyll is a pigment, not something that is converted by sunlight.\n",
      "\n",
      "### Correct Answer:\n",
      "$$\n",
      "\\boxed{C}\n",
      "$$\n",
      "**********************\n",
      "Dataset: test-ai2_arc.parquet\n",
      "âœ… Total Questions: 3\n",
      "âœ… Correct Answers: 3\n",
      "ðŸ“Š Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "models = [\"Qwen/Qwen3-1.7B\"]\n",
    "datasets = list(Path(\"../Datasets\").glob('*.parquet'))\n",
    "\n",
    "for model_name in models:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=device_map\n",
    "    )\n",
    "\n",
    "    print(f\"**********************\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "\n",
    "    for dataset in datasets:\n",
    "        dataset_name = dataset.name\n",
    "\n",
    "        df = pd.read_parquet(dataset).head(3)\n",
    "        total = len(df)\n",
    "        correct_count = 0\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            user_prompt = row[\"question\"]\n",
    "\n",
    "            if datasets_info[dataset_name][\"context\"]:\n",
    "                user_prompt += row[\"context\"]\n",
    "\n",
    "            response = chat(model, tokenizer, user_prompt, datasets_info[dataset_name][\"system_prompt\"])\n",
    "\n",
    "            is_correct = evaluate_answer(response, str(row[\"answer\"]))\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "\n",
    "        # --- Final stats ---\n",
    "        accuracy = correct_count / total * 100\n",
    "        print(f\"**********************\")\n",
    "        print(f\"Dataset: {dataset_name}\")\n",
    "        print(f\"âœ… Total Questions: {total}\")\n",
    "        print(f\"âœ… Correct Answers: {correct_count}\")\n",
    "        print(f\"ðŸ“Š Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-training (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
