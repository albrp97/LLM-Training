{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo fusionado guardado con metadata en: Qwen2.5-Coder-3B-Instruct-JavaSpring-trained-merged/model.safetensors\n",
      "Archivo copiado: tokenizer_config.json\n",
      "Archivo copiado: tokenizer.json\n",
      "Archivo copiado: vocab.json\n",
      "Archivo copiado: special_tokens_map.json\n",
      "Archivo copiado: added_tokens.json\n",
      "Archivo copiado: merges.txt\n",
      "Archivo copiado: README.md\n",
      "Archivo copiado: config.json\n",
      "Archivo copiado: generation_config.json\n",
      "Todos los archivos necesarios han sido copiados a: Qwen2.5-Coder-3B-Instruct-JavaSpring-trained-merged\n",
      "Tokenizer re-saved to: Qwen2.5-Coder-3B-Instruct-JavaSpring-trained-merged\n",
      "Advertencia: config.json no existe en el modelo original y no se pudo copiar. Descargar a mano desde el modelo original Qwen/Qwen2.5-Coder-3B-Instruct.\n",
      "Tokenizer re-saved to: Qwen2.5-Coder-3B-Instruct-JavaSpring-trained-merged\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import safetensors.torch\n",
    "from transformers import AutoTokenizer\n",
    "os.chdir(\"/app\")\n",
    "\n",
    "# Original tokenizer path\n",
    "original_model_path = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
    "# Rutas de entrada y salida\n",
    "base_model_path = \"Qwen2.5-Coder-3B-Instruct-JavaSpring-trained\"\n",
    "output_model_path = \"Qwen2.5-Coder-3B-Instruct-JavaSpring-trained-merged\"\n",
    "\n",
    "# Crear directorio de salida si no existe\n",
    "os.makedirs(output_model_path, exist_ok=True)\n",
    "\n",
    "# Cargar el modelo base\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
    "\n",
    "# Cargar y fusionar el adaptador LoRA\n",
    "adapter_folder = base_model_path  # La carpeta del adaptador\n",
    "model = PeftModel.from_pretrained(model, adapter_folder)\n",
    "model = model.merge_and_unload()  # Fusionar LoRA con el modelo base\n",
    "\n",
    "# Resolver la compartición de tensores\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Clonar tensores compartidos\n",
    "if \"model.embed_tokens.weight\" in state_dict and \"lm_head.weight\" in state_dict:\n",
    "    if state_dict[\"model.embed_tokens.weight\"].data_ptr() == state_dict[\"lm_head.weight\"].data_ptr():\n",
    "        state_dict[\"lm_head.weight\"] = state_dict[\"lm_head.weight\"].clone()\n",
    "\n",
    "# Guardar el modelo completo en formato safetensors con metadata\n",
    "output_safetensors_path = os.path.join(output_model_path, \"model.safetensors\")\n",
    "metadata = {\"format\": \"pt\"}  # Add metadata indicating PyTorch format\n",
    "safetensors.torch.save_file(state_dict, output_safetensors_path, metadata=metadata)\n",
    "print(f\"Modelo fusionado guardado con metadata en: {output_safetensors_path}\")\n",
    "\n",
    "# Copiar otros archivos del modelo original a la nueva carpeta\n",
    "files_to_copy = [\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"vocab.json\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"added_tokens.json\",\n",
    "    \"merges.txt\",\n",
    "    \"README.md\", \n",
    "    \"config.json\",\n",
    "    \"generation_config.json\",\n",
    "]\n",
    "\n",
    "for file in files_to_copy:\n",
    "    src = os.path.join(base_model_path, file)\n",
    "    dst = os.path.join(output_model_path, file)\n",
    "    if os.path.exists(src):\n",
    "        with open(src, \"rb\") as fsrc, open(dst, \"wb\") as fdst:\n",
    "            fdst.write(fsrc.read())\n",
    "            print(f\"Archivo copiado: {file}\")\n",
    "\n",
    "print(f\"Todos los archivos necesarios han sido copiados a: {output_model_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Reload and save tokenizer and config.json from the original model path\n",
    "try:\n",
    "    # Load the tokenizer from the original model path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(original_model_path)\n",
    "    tokenizer.save_pretrained(output_model_path)\n",
    "    print(f\"Tokenizer re-saved to: {output_model_path}\")\n",
    "    \n",
    "    # Load and save the config.json from the original model path\n",
    "    config_src = os.path.join(original_model_path, \"config.json\")\n",
    "    config_dst = os.path.join(output_model_path, \"config.json\")\n",
    "    if os.path.exists(config_src):\n",
    "        with open(config_src, \"rb\") as fsrc, open(config_dst, \"wb\") as fdst:\n",
    "            fdst.write(fsrc.read())\n",
    "            print(f\"Archivo copiado: config.json\")\n",
    "    else:\n",
    "        print(f\"Advertencia: config.json no existe en el modelo original y no se pudo copiar. Descargar a mano desde el modelo original {original_model_path}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al recargar o guardar el tokenizer y config.json: {str(e)}\")\n",
    "\n",
    "print(f\"Tokenizer re-saved to: {output_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
