{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/app\")\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El modelo que se quiere afinar desde el hub de Hugging Face\n",
    "# Se seleccionan dos grandes modelos, ambos de alrededor de 7B-8B parámetros, lo que afecta directamente al uso de VRAM.\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
    "\n",
    "# El dataset de instrucciones a utilizar para el ajuste fino\n",
    "# Afecta a la tarea que el modelo realizará, en este caso, instrucciones.\n",
    "local_dataset_path = \"angular_dataset.csv\"\n",
    "\n",
    "# Nombre del modelo ajustado\n",
    "new_model = f\"{model_name}-angular-trained\"\n",
    "\n",
    "model_layers = ['q_proj','k_proj','v_proj','o_proj']\n",
    "\n",
    "################################################################################\n",
    "# Parámetros de QLoRA (Fine-tuning con LoRA de bajo rango)\n",
    "################################################################################\n",
    "# LoRA (Adaptación de Bajo Rango) es una técnica utilizada para reducir el número de parámetros que necesitan ser ajustados durante el fine-tuning.\n",
    "# En lugar de actualizar todos los parámetros del modelo, LoRA inserta capas de bajo rango en matrices de proyección de atención. \n",
    "# Esto permite un ajuste eficiente con menor uso de VRAM, ya que solo se entrenan estas capas adicionales.\n",
    "# En términos de memoria y tiempo de cómputo, manteniendo gran parte de los parámetros originales congelados.\n",
    "\n",
    "# Dimensión de la atención en LoRA\n",
    "# Determina el tamaño del cuello de botella en LoRA; valores más altos requieren más VRAM pero permiten capturar más información.\n",
    "lora_r = 32\n",
    "\n",
    "# Parámetro Alpha para el escalado de LoRA\n",
    "# Afecta la intensidad del ajuste fino aplicado; valores más altos pueden mejorar el rendimiento pero aumentan el uso de memoria.\n",
    "lora_alpha = 16\n",
    "\n",
    "# Probabilidad de Dropout en las capas LoRA\n",
    "# Dropout introduce regularización, reduciendo el sobreajuste y el uso de VRAM en la activación de capas.\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# Parámetros de bitsandbytes (Quantización)\n",
    "################################################################################\n",
    "# La cuantización es una técnica utilizada para reducir el tamaño de los modelos y el uso de VRAM al representar los pesos del modelo con menor precisión, \n",
    "# por ejemplo, usando 4 bits en lugar de 16 o 32. \n",
    "# Esto permite cargar modelos más grandes en GPUs con menos memoria. \n",
    "# Aunque puede introducir una leve pérdida de precisión, la cuantización permite hacer más eficiente el entrenamiento y la inferencia.\n",
    "# En esta configuración se utiliza la cuantización de 4 bits (opcional), y se pueden usar diferentes tipos como fp4 y nf4, que ofrecen un buen \n",
    "# balance entre eficiencia y rendimiento.\n",
    "\n",
    "# Activar carga de modelos base con precisión de 4 bits - Necesita GPU\n",
    "# Reduce el uso de VRAM y permite entrenar modelos más grandes en GPUs con menos memoria.\n",
    "use_4bit = False\n",
    "\n",
    "# Tipo de datos de cómputo para modelos base de 4 bits\n",
    "# El uso de float16 reduce el uso de VRAM en comparación con float32.\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Tipo de cuantización (fp4 o nf4)\n",
    "# nf4 es más eficiente en términos de memoria y precisión en comparación con fp4.\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activar cuantización anidada para modelos base de 4 bits (cuantización doble)\n",
    "# Aumenta la compresión, reduciendo aún más la VRAM utilizada, pero puede afectar el rendimiento.\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# Parámetros de TrainingArguments (Ajustes de entrenamiento)\n",
    "################################################################################\n",
    "\n",
    "# Directorio de salida donde se almacenarán las predicciones y checkpoints del modelo\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Número de épocas de entrenamiento\n",
    "# Afecta la duración del entrenamiento; más épocas pueden mejorar la generalización pero requieren más tiempo.\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Activar entrenamiento con fp16/bf16 (usar bf16 si se tiene una GPU A100/H100)\n",
    "# fp16/bf16 reduce significativamente el uso de VRAM, lo que permite entrenar modelos más grandes.\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "\n",
    "# Tamaño del batch por GPU para entrenamiento y evaluación\n",
    "# Un mayor tamaño de batch incrementa el uso de VRAM, pero puede acelerar el entrenamiento si la memoria lo permite.\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Número de pasos de actualización para acumular los gradientes\n",
    "# Aumenta el uso de memoria temporalmente pero permite actualizar los gradientes menos frecuentemente.\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Activar checkpointing de gradientes\n",
    "# Reduce el uso de VRAM al recalcular activaciones durante el retropropagado, a cambio de un mayor tiempo de cómputo.\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Máxima norma del gradiente (clipping)\n",
    "# Previene explosiones de gradiente, especialmente útil en grandes modelos, pero afecta el uso de memoria.\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Tasa de aprendizaje inicial (optimizador AdamW)\n",
    "# Controla la velocidad de convergencia, un valor más alto puede requerir mayor memoria debido a la intensidad de los ajustes.\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Decaimiento del peso para todas las capas excepto bias/pesos de LayerNorm\n",
    "# Afecta la regularización durante el entrenamiento, influyendo en cómo se ajustan los pesos del modelo.\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizador a usar\n",
    "# En caso de utilizar un modelo de 4 bits, se utiliza paged_adamw_32bit, optimizado para la eficiencia de memoria.\n",
    "optim = \"paged_adamw_32bit\" if use_4bit else \"adamw_torch\"\n",
    "\n",
    "# Programador de tasa de aprendizaje\n",
    "# El programador \"cosine\" ajusta la tasa de aprendizaje de forma gradual, lo que puede afectar el tiempo de convergencia y el uso de memoria.\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Número máximo de pasos de entrenamiento (anula num_train_epochs si se establece)\n",
    "# Controla cuántos pasos de entrenamiento ejecutar; valores mayores requieren más tiempo y uso de VRAM.\n",
    "max_steps = -1\n",
    "\n",
    "# Proporción de pasos para un calentamiento lineal (de 0 a la tasa de aprendizaje)\n",
    "# Un mayor warmup suaviza el inicio del entrenamiento, lo que puede ayudar a estabilizar los gradientes en grandes modelos.\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Agrupar secuencias en batches con la misma longitud\n",
    "# Ahorra memoria y acelera el entrenamiento al minimizar el relleno innecesario en secuencias más cortas.\n",
    "group_by_length = True\n",
    "\n",
    "# Guardar un checkpoint cada X pasos de actualización\n",
    "# Afecta la frecuencia con la que se almacenan los modelos; más checkpoints requieren más espacio de almacenamiento pero permiten recuperar más fácilmente.\n",
    "save_steps = 0\n",
    "\n",
    "# Registrar logs cada X pasos de actualización\n",
    "# Afecta la frecuencia de logging, sin impacto directo en la VRAM pero sí en el rendimiento.\n",
    "logging_steps = 50\n",
    "\n",
    "################################################################################\n",
    "# Parámetros de SFT (Supervised Fine-Tuning)\n",
    "################################################################################\n",
    "\n",
    "# Longitud máxima de secuencia\n",
    "# Limita la longitud de las secuencias procesadas; secuencias más largas usan más VRAM.\n",
    "max_seq_length = 1024\n",
    "\n",
    "# Empaquetar múltiples ejemplos cortos en la misma secuencia de entrada para aumentar la eficiencia\n",
    "# Mejora la eficiencia del entrenamiento, agrupando ejemplos cortos para reducir el relleno y ahorrar VRAM.\n",
    "packing = True\n",
    "\n",
    "# Cargar el modelo entero en la GPU 0\n",
    "# Especifica en qué dispositivo se carga el modelo; si es GPU, afectará el uso de VRAM.\n",
    "device_map = {\"\": 0} if torch.cuda.is_available() else {\"\": \"cpu\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 220 examples [00:00, 7273.97 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 220\n",
      "})\n",
      "guide accessibility\n",
      "# Accessibility in Angular\n",
      "\n",
      "The web is used by a wide variety of people, including those who have visual or motor impairments. A variety of assistive technologies are available that make it much easier for these groups to interact with web-based software applications. Also, designing an application to be more accessible generally improves the user experience for all users.\n",
      "\n",
      "For an in-depth introduction to issues and techniques for designing accessible applications, see the Accessibility section of the Google's Web Fundamentals.\n",
      "\n",
      "This page discusses best practices for designing Angular applications that work well for all users, including those who rely on assistive technologies.\n",
      "\n",
      "> For the sample application that this page describes, see the live example.\n",
      "\n",
      "## Accessibility attributes\n",
      "\n",
      "Building accessible web experience often involves setting Accessible Rich Internet Applications (ARIA) attributes to provide semantic meaning where it might otherwise be missing. Use attribute binding template syntax to control the values of accessibility-related attributes.\n",
      "\n",
      "When binding to ARIA attributes in Angular, you must use the `attr.` prefix. The ARIA specification depends specifically on HTML attributes rather than properties of DOM elements.\n",
      "\n",
      "```\n",
      "\n",
      "…\n",
      "```\n",
      "> **NOTE** This syntax is only necessary for attribute *bindings*. Static ARIA attributes require no extra syntax.\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> …\n",
      "> ```\n",
      "\n",
      "> By convention, HTML attributes use lowercase names (`tabindex`), while properties use camelCase names (`tabIndex`).\n",
      "> \n",
      "> See the Binding syntax guide for more background on the difference between attributes and properties.\n",
      "\n",
      "## Angular UI components\n",
      "\n",
      "The Angular Material library, which is maintained by the Angular team, is a suite of reusable UI components that aims to be fully accessible. The Component Development Kit (CDK) includes the `a11y` package that provides tools to support various areas of accessibility. For example:\n",
      "\n",
      "* `LiveAnnouncer` is used to announce messages for screen-reader users using an `aria-live` region. See the W3C documentation for more information on aria-live regions.\n",
      "* The `cdkTrapFocus` directive traps Tab-key focus within an element. Use it to create accessible experience for components such as modal dialogs, where focus must be constrained.\n",
      "\n",
      "For full details of these and other tools, see the Angular CDK accessibility overview.\n",
      "\n",
      "### Augmenting native elements\n",
      "\n",
      "Native HTML elements capture several standard interaction patterns that are important to accessibility. When authoring Angular components, you should re-use these native elements directly when possible, rather than re-implementing well-supported behaviors.\n",
      "\n",
      "For example, instead of creating a custom element for a new variety of button, create a component that uses an attribute selector with a native `` element. This most commonly applies to `` and ``, but can be used with many other types of element.\n",
      "\n",
      "You can see examples of this pattern in Angular Material: `MatButton`, `MatTabNav`, and `MatTable`.\n",
      "\n",
      "### Using containers for native elements\n",
      "\n",
      "Sometimes using the appropriate native element requires a container element. For example, the native `` element cannot have children, so any custom text entry components need to wrap an `` with extra elements. By just including `` in your custom component's template, it's impossible for your component's users to set arbitrary properties and attributes to the `` element. Instead, create a container component that uses content projection to include the native control in the component's API.\n",
      "\n",
      "You can see `MatFormField` as an example of this pattern.\n",
      "\n",
      "## Case study: Building a custom progress bar\n",
      "\n",
      "The following example shows how to make a progress bar accessible by using host binding to control accessibility-related attributes.\n",
      "\n",
      "* The component defines an accessibility-enabled element with both the standard HTML attribute `role`, and ARIA attributes. The ARIA attribute `aria-valuenow` is bound to the user's input.\n",
      "  \n",
      "  ```\n",
      "  import { Component, Input } from '@angular/core';\n",
      "  \n",
      "  /**\n",
      "   * Example progressbar component.\n",
      "   */\n",
      "  @Component({\n",
      "    selector: 'app-example-progressbar',\n",
      "    template: '',\n",
      "    styleUrls: ['./progress-bar.component.css'],\n",
      "    host: {\n",
      "      // Sets the role for this component to \"progressbar\"\n",
      "      role: 'progressbar',\n",
      "  \n",
      "      // Sets the minimum and maximum values for the progressbar role.\n",
      "      'aria-valuemin': '0',\n",
      "      'aria-valuemax': '100',\n",
      "  \n",
      "      // Binding that updates the current value of the progressbar.\n",
      "      '[attr.aria-valuenow]': 'value',\n",
      "    }\n",
      "  })\n",
      "  export class ExampleProgressbarComponent  {\n",
      "    /** Current value of the progressbar. */\n",
      "    @Input() value = 0;\n",
      "  }\n",
      "  ```\n",
      "* In the template, the `aria-label` attribute ensures that the control is accessible to screen readers.\n",
      "  \n",
      "  ```\n",
      "  \n",
      "    Enter an example progress value\n",
      "    \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  ```\n",
      "\n",
      "## Routing\n",
      "\n",
      "### Focus management after navigation\n",
      "\n",
      "Tracking and controlling focus in a UI is an important consideration in designing for accessibility. When using Angular routing, you should decide where page focus goes upon navigation.\n",
      "\n",
      "To avoid relying solely on visual cues, you need to make sure your routing code updates focus after page navigation. Use the `NavigationEnd` event from the `Router` service to know when to update focus.\n",
      "\n",
      "The following example shows how to find and focus the main content header in the DOM after navigation.\n",
      "\n",
      "```\n",
      "router.events.pipe(filter(e => e instanceof NavigationEnd)).subscribe(() => {\n",
      "  const mainHeader = document.querySelector('#main-content-header')\n",
      "  if (mainHeader) {\n",
      "    mainHeader.focus();\n",
      "  }\n",
      "});\n",
      "```\n",
      "\n",
      "In a real application, the element that receives focus depends on your specific application structure and layout. The focused element should put users in a position to immediately move into the main content that has just been routed into view. You should avoid situations where focus returns to the `body` element after a route change.\n",
      "\n",
      "### Active links identification\n",
      "\n",
      "CSS classes applied to active `RouterLink` elements, such as `RouterLinkActive`, provide a visual cue to identify the active link. Unfortunately, a visual cue doesn't help blind or visually impaired users. Applying the `aria-current` attribute to the element can help identify the active link. For more information, see Mozilla Developer Network (MDN) aria-current).\n",
      "\n",
      "The `RouterLinkActive` directive provides the `ariaCurrentWhenActive` input which sets the `aria-current` to a specified value when the link becomes active.\n",
      "\n",
      "The following example shows how to apply the `active-page` class to active links as well as setting their `aria-current` attribute to `\"page\"` when they are active:\n",
      "\n",
      "```\n",
      "\n",
      "      \n",
      "        Home\n",
      "      \n",
      "      \n",
      "        About\n",
      "      \n",
      "      \n",
      "        Shop\n",
      "      \n",
      "    \n",
      "```\n",
      "## More information\n",
      "\n",
      "* Accessibility - Google Web Fundamentals\n",
      "* ARIA specification and authoring practices\n",
      "* Material Design - Accessibility\n",
      "* Smashing Magazine\n",
      "* Inclusive Components\n",
      "* Accessibility Resources and Code Examples\n",
      "* W3C - Web Accessibility Initiative\n",
      "* Rob Dodson A11ycasts\n",
      "* Angular ESLint provides linting rules that can help you make sure your code meets accessibility standards.\n",
      "\n",
      "Books\n",
      "\n",
      "* \"A Web for Everyone: Designing Accessible User Experiences,\" Sarah Horton and Whitney Quesenbery\n",
      "* \"Inclusive Design Patterns,\" Heydon Pickering\n",
      "\n",
      "Last reviewed on Mon Feb 28 2022\n",
      "\n",
      "© 2010–2023 Google, Inc.  \n",
      "Licensed under the Creative Commons Attribution License 4.0.  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Back\n",
      "\n",
      " Apply\n",
      "\n",
      "DocsSettings\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##### Tracking cookies\n",
      "\n",
      "We would like to gather usage data about how DevDocs is used through Google Analytics and Gauges. We only collect anonymous traffic information.\n",
      "Please confirm if you accept our tracking cookies. You can always change your decision in the settings.\n",
      "  \n",
      "Accept or Decline\n",
      "Close\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [02:26<00:00, 73.24s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.66s/it]\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Generating train split: 487 examples [00:00, 1378.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "Qwen2ForCausalLM                                   [1, 512, 151936]          --\n",
      "├─Qwen2Model: 1-1                                  [1, 512, 2048]            --\n",
      "│    └─Embedding: 2-1                              [1, 512, 2048]            (311,164,928)\n",
      "│    └─Qwen2RotaryEmbedding: 2-2                   [1, 512, 128]             --\n",
      "│    └─ModuleList: 2-3                             --                        --\n",
      "│    │    └─Qwen2DecoderLayer: 3-1                 [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-2                 [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-3                 [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-4                 [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-5                 [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-6                 [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-7                 [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-8                 [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-9                 [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-10                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-11                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-12                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-13                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-14                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-15                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-16                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-17                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-18                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-19                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-20                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-21                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-22                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-23                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-24                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-25                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-26                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-27                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-28                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-29                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-30                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-31                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-32                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-33                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-34                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-35                [1, 512, 2048]            77,486,592\n",
      "│    │    └─Qwen2DecoderLayer: 3-36                [1, 512, 2048]            77,486,592\n",
      "│    └─Qwen2RMSNorm: 2-4                           [1, 512, 2048]            (2,048)\n",
      "├─Linear: 1-2                                      [1, 512, 151936]          (311,164,928)\n",
      "====================================================================================================\n",
      "Total params: 3,411,849,216\n",
      "Trainable params: 14,745,600\n",
      "Non-trainable params: 3,397,103,616\n",
      "Total mult-adds (G): 3.41\n",
      "====================================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 6169.30\n",
      "Params size (MB): 13647.40\n",
      "Estimated Total Size (MB): 19816.70\n",
      "====================================================================================================\n",
      "Model Summary:\n",
      "Model name: Qwen/Qwen2.5-Coder-3B-Instruct\n",
      "Number of parameters: 3.10 billion\n",
      "Number of trainable parameters: 0.01 billion\n",
      "Percentage of trainable parameters: 0.48%\n",
      "Number of layers: 36\n",
      "Hidden size: 2048\n",
      "=========================================================================================================\n",
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA H100 NVL MIG 3g.47gb\n",
      "Using GPU: NVIDIA H100 NVL MIG 3g.47gb\n",
      "Total VRAM: 46.38 GB\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset (puedes procesarlo aquí)\n",
    "# El dataset se carga desde Hugging Face, usando el nombre especificado en \"dataset_name\".\n",
    "dataset = load_dataset('csv', data_files=local_dataset_path, split='train')\n",
    "print(dataset)\n",
    "print(dataset['text'][0])\n",
    "\n",
    "# Cargar el tokenizer y el modelo con la configuración de QLoRA\n",
    "# Se selecciona el tipo de datos de cómputo (float16 en este caso) para el modelo.\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# Si se activa el uso de 4 bits, se configura la cuantización BitsAndBytes (para reducir el uso de VRAM).\n",
    "if use_4bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=use_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=use_nested_quant,\n",
    "    )\n",
    "else:\n",
    "    quantization_config = None\n",
    "\n",
    "# Verifica la compatibilidad de la GPU con bfloat16 (mejora en entrenamiento y uso de memoria)\n",
    "if torch.cuda.is_available() and compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "# Desactiva el uso de cache para reducir la memoria durante el entrenamiento\n",
    "model.config.use_cache = False \n",
    "# Controla el particionamiento en modelos grandes para paralelización\n",
    "model.config.pretraining_tp = 1  \n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Cargar la configuración de LoRA (Low-Rank Adaptation)\n",
    "# LoRA permite ajustar solo ciertas partes del modelo (proyectores de atención) para ahorrar memoria.\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=model_layers\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Calculate total and trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "percentage_trainable = (trainable_params / total_params) * 100\n",
    "\n",
    "# Print a detailed summary using torchinfo\n",
    "if not use_4bit:\n",
    "    print(summary(model, input_size=(1, 512), dtypes=[torch.int]))\n",
    "\n",
    "# Print a summary of the model\n",
    "print(\"Model Summary:\")\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"Number of parameters: {total_params / 1e9:.2f} billion\")\n",
    "print(f\"Number of trainable parameters: {trainable_params / 1e9:.2f} billion\")\n",
    "print(f\"Percentage of trainable parameters: {percentage_trainable:.2f}%\")\n",
    "print(f\"Number of layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "\n",
    "print('=========================================================================================================')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i}: {gpu_name}\")\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    current_gpu_name = torch.cuda.get_device_name(device.index)\n",
    "    print(f\"Using GPU: {current_gpu_name}\")\n",
    "\n",
    "    memory_stats = torch.cuda.memory_stats(device)\n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "    allocated_memory = memory_stats['allocated_bytes.all.current']\n",
    "    reserved_memory = memory_stats['reserved_bytes.all.current']\n",
    "    free_memory = total_memory - reserved_memory\n",
    "\n",
    "    print(f\"Total VRAM: {total_memory / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print('=========================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create an Angular service named `AuthService` that contains a method for logging in users, you can follow these steps:\n",
      "\n",
      "1. **Create the AuthService Class**:\n",
      "   - Open your Angular project and navigate to the `src/app` directory.\n",
      "   - Create a new file named `auth.service.ts`.\n",
      "   - Add the following code to define the `AuthService` class:\n",
      "\n",
      "```typescript\n",
      "import { Injectable } from '@angular/core';\n",
      "\n",
      "@Injectable({\n",
      "  providedIn: 'root'\n",
      "})\n",
      "export class AuthService {\n",
      "  private isLoggedIn = false;\n",
      "\n",
      "  // Method to simulate user login\n",
      "  login(username: string, password: string): boolean {\n",
      "    // For demonstration purposes, let's assume the username is \"admin\" and password is \"password\"\n",
      "    if (username === \"admin\" && password === \"password\") {\n",
      "      this.isLoggedIn = true;\n",
      "      return true;\n",
      "    }\n",
      "    return false;\n",
      "  }\n",
      "\n",
      "  // Method to check if the user is logged in\n",
      "  isLoggedIn(): boolean {\n",
      "    return this\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"To create an Angular service named `AuthService` that contains a method\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"{prompt}\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='122' max='122' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [122/122 01:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.526400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=122, training_loss=1.5708340191450276, metrics={'train_runtime': 61.5261, 'train_samples_per_second': 7.915, 'train_steps_per_second': 1.983, 'total_flos': 8346598983598080.0, 'train_loss': 1.5708340191450276, 'epoch': 1.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Qwen/Qwen2.5-Coder-3B-Instruct-angular-trained/tokenizer_config.json',\n",
       " 'Qwen/Qwen2.5-Coder-3B-Instruct-angular-trained/special_tokens_map.json',\n",
       " 'Qwen/Qwen2.5-Coder-3B-Instruct-angular-trained/vocab.json',\n",
       " 'Qwen/Qwen2.5-Coder-3B-Instruct-angular-trained/merges.txt',\n",
       " 'Qwen/Qwen2.5-Coder-3B-Instruct-angular-trained/added_tokens.json',\n",
       " 'Qwen/Qwen2.5-Coder-3B-Instruct-angular-trained/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(new_model)\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create an Angular service named `AuthService` that contains a method to authenticate users, you can follow these steps:\n",
      "\n",
      "1. **Install Angular CLI**: If you haven't already, install the Angular CLI globally on your machine by running:\n",
      "   ```bash\n",
      "   npm install -g @angular/cli\n",
      "   ```\n",
      "\n",
      "2. **Create a New Angular Project**: If you don't have an existing project, create a new one using the Angular CLI:\n",
      "   ```bash\n",
      "   ng new my-angular-app\n",
      "   cd my-angular-app\n",
      "   ```\n",
      "\n",
      "3. **Generate a Service**: Use the Angular CLI to generate a new service named `AuthService`:\n",
      "   ```bash\n",
      "   ng generate service auth\n",
      "   ```\n",
      "\n",
      "4. **Implement the AuthService**: Open the `auth.service.ts` file and implement the authentication logic. Here's a simple example using HTTP requests:\n",
      "\n",
      "   ```typescript\n",
      "   import { Injectable } from '@angular/core';\n",
      "   import { HttpClient } from '@angular/common/http';\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "pipe = pipeline(task=\"text-generation\", model=new_model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"{prompt}\")\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
