@Lobato and I have been researching **how to make local models truly feasible**â€”and answering a key question when building LLM solutions: **do we need big API models, or can a small, fine-tuned local model deliver better value?** ğŸ¤”ğŸ’¸ğŸš€

### TL;DR

* **API â‰  automatic best choice.** With a **locally fine-tuned 0.6B**, we reached **quality comparable to a larger 1.7B base** while cutting **latency by up to ~99%**, avoiding API price swings and rate limits. **More control, lower opex, predictable governance.** ğŸ”’âš¡ğŸ“‰
* **Task fit matters.** **Context-anchored tasks** (RAG/extractive QA) gain the most from SFT; **math/logic-heavy tasks** benefit less from size-preserving SFT.
* **Cross-task effects:** full FT can yield strong **positive transfer** or **catastrophic forgetting**â€”plan training and evaluation with that in mind. ğŸ§ª
* **Full fine-tuning > LoRA** when VRAM allowsâ€”consistently **better efficacy** and **often faster** in our runs. ğŸï¸ğŸ“ˆ
* **LoRA rank:** bigger rank **didnâ€™t consistently help**. If you must use LoRA, **~256** worked well in practice. ğŸ§©

### What we tested ğŸ§ª

* **Models:** Qwen3-0.6B (base, LoRA râˆˆ{32â€¦1024}, full FT) and Qwen3-1.7B (base). ğŸ¤–
* **Tasks:** ARC (MCQ), OpenMathInstruct-2 (numeric), SQuAD v2 (extractive QA). ğŸ§ ğŸ“š
* **Procedure:** Supervised FT (TRL/Transformers), deterministic decoding, reporting **quality + wall-clock latency** per sample. ğŸ§¬â±ï¸

### Highlights âœ¨

* **Local 0.6B + full FT** vs 0.6B base: ğŸ âš¡

  * **Latency:** â†“ up to **~99.2%** (e.g., numeric tasks) and **~94.7%** on MCQ. â±ï¸â¬‡ï¸
  * **Quality:** **SQuAD v2 F1** â†‘ **~177.6%** when tuned on SQuAD; **ARC macro-F1** â†‘ **~4.8%** when tuned on OpenMath (transfer effect). ğŸ“ˆğŸ†
* **Full FT vs best LoRA:** ğŸ†š

  * On numeric tasks, full FT cut error by **~7,115 absolute units** vs best LoRA **and** ran **~1.45s faster** per sample. ğŸ”¢âš¡
  * On extractive QA, full FT beat best LoRA by **+18.36 F1**, with a tiny latency trade-off (~0.03s). ğŸ“ğŸ‘Œ
* **LoRA rank:** Correlations with score were ~0 or inconsistent; **use râ‰ˆ256** by default if constrained. ğŸšï¸âœ…
* **Transfer & forgetting:** ğŸ”ğŸ§ 

  * **Positive transfer:** ARC-tuned â†’ SQuAD saw **large F1 gains** in our setting. â•ğŸš€
  * **Forgetting:** OpenMath-tuned â†’ SQuAD showed **~26.5% F1 drop** (single-task FT can erode other skills). âš ï¸ğŸ“‰

### Practical takeaways ğŸ’¡

* **If you rely on APIs today:** evaluate a **small, locally fine-tuned model** firstâ€”youâ€™ll likely **save cost**, **cut latency**, and **gain control** over versions/compliance. ğŸ’¸ âš¡
* **When VRAM is available:** prefer **full FT** over adapters for best results. ğŸ–¥ï¸ âœ…
* **When VRAM is tight or you need multiple domains:** use **LoRA** (hot-swappable adapters), and start at **râ‰ˆ256**. ğŸ” ğŸšï¸
* **Match FT to task type:** ğŸ¯

  * **Best with FT:** context-anchored/RAG/extractive QA (formatting, span selection, instruction alignment). ğŸ“š ğŸ”—
  * **Less uplift from size-preserving FT:** math/logic-heavy workloadsâ€”consider scaling base size or complementary techniques. ğŸ§® â¬†ï¸

Weâ€™ll keep publishing new experiments on **fine-tuning strategies**, **quantization**, and **inference optimizations** to push smaller models further. ğŸ”¬

ğŸ”— **Repo:** <add your repo link>
ğŸ“” **Complete journal (PDF):** <add your journal link>

â€”
#LLM #FineTuning #LoRA #PEFT #Latency #CostOptimization #RAG #MLOps #OpenSource #Qwen #NLP #AIEngineering #AI #MachineLearning #OnDeviceAI #EdgeAI #PrivateAI #Quantization #ModelCompression #Distillation #Inference #ModelServing #LatencyReduction #CostSavings #Governance #LLMEval #Benchmarking
