{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23e7b6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìÑ Dataset: train-ai2_arc.parquet\n",
      "üî¢ Rows: 1,119 | Columns: 3\n",
      "\n",
      "üß™ Sample:\n",
      "                    title                                           question  \\\n",
      "243       Mercury_7030083  The day before the class is going to do a lab ...   \n",
      "101     Mercury_SC_401589  Which mixture contains ingredients that can be...   \n",
      "961        Mercury_417154  In 2005, a team of scientists discovered a pho...   \n",
      "1060  Mercury_SC_LBS10384  Vegetables can be scientifically classified by...   \n",
      "522    TIMSS_2007_8_pg128  A sound is heard when you pluck a string on a ...   \n",
      "\n",
      "     answer  \n",
      "243       B  \n",
      "101       B  \n",
      "961       B  \n",
      "1060      D  \n",
      "522       B  \n",
      "\n",
      "üíæ Overwritten (sampled): ..\\Datasets\\train-ai2_arc.parquet with 1000 rows.\n",
      "\n",
      "================================================================================\n",
      "üìÑ Dataset: train-boolq.parquet\n",
      "üî¢ Rows: 9,427 | Columns: 3\n",
      "\n",
      "üß™ Sample:\n",
      "                                               question  answer  \\\n",
      "8681     did jurassic world fallen kingdom come out yet    True   \n",
      "2362  has there ever been a host team in the super bowl   False   \n",
      "6232      is it legal to have a radar detector in texas    True   \n",
      "1318            is chitty chitty bang bang just a story    True   \n",
      "543   has anyone won the grand slam in golf in one year    True   \n",
      "\n",
      "                                                context  \n",
      "8681  Filming took place from February to July 2017 ...  \n",
      "2362  The home field curse affects the host team of ...  \n",
      "6232  Radar jammers are not allowed under federal ru...  \n",
      "1318  Chitty Chitty Bang Bang is a 1968 British musi...  \n",
      "543   The term ``Grand Slam'' was first applied to B...  \n",
      "\n",
      "üíæ Overwritten (sampled): ..\\Datasets\\train-boolq.parquet with 1000 rows.\n",
      "\n",
      "================================================================================\n",
      "üìÑ Dataset: train-squad_v2.parquet\n",
      "üî¢ Rows: 130,319 | Columns: 4\n",
      "\n",
      "üß™ Sample:\n",
      "                                      title  \\\n",
      "125137  Financial_crisis_of_2007%E2%80%9308   \n",
      "30275                           House_music   \n",
      "39176                Mary_(mother_of_Jesus)   \n",
      "32129                      Himachal_Pradesh   \n",
      "44136                          Elizabeth_II   \n",
      "\n",
      "                                                  context  \\\n",
      "125137  It threatened the collapse of large financial ...   \n",
      "30275   But house was also being developed on Ibiza,[c...   \n",
      "39176   Although Calvin and Huldrych Zwingli honored M...   \n",
      "32129   Due to extreme variation in elevation, great v...   \n",
      "44136   The Queen addressed the United Nations for a s...   \n",
      "\n",
      "                                                 question  \\\n",
      "125137  What year did the global recession that follow...   \n",
      "30275   what was a popular club in ibiza that started ...   \n",
      "39176   In what century did Martin Luther honor Mary a...   \n",
      "32129                           What is the climate like?   \n",
      "44136         How many times has the Queen toured Canada?   \n",
      "\n",
      "                                       answer  \n",
      "125137                                   2012  \n",
      "30275                                 Amnesia  \n",
      "39176                                          \n",
      "32129   varies from hot and subhumid tropical  \n",
      "44136                                          \n",
      "\n",
      "üíæ Overwritten (sampled): ..\\Datasets\\train-squad_v2.parquet with 1000 rows.\n",
      "\n",
      "================================================================================\n",
      "üìÑ Dataset: train-OpenMathInstruct-2.parquet\n",
      "üî¢ Rows: 5,590 | Columns: 2\n",
      "\n",
      "üß™ Sample:\n",
      "                                               question   answer\n",
      "4865  A bookstore ordered 200 more than three times ...  19600.0\n",
      "3433  In triangle $ABC$, $AB$ is congruent to $AC$, ...      7.0\n",
      "4666  Find the smallest integer greater than 85 that...     94.0\n",
      "5523  What is the remainder when $5^{207}$ is divide...      6.0\n",
      "2110  A water tank can be filled by two pipes, one f...    135.0\n",
      "\n",
      "üíæ Overwritten (sampled): ..\\Datasets\\train-OpenMathInstruct-2.parquet with 1000 rows.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reduce and overwrite TRAIN datasets.\n",
    "\n",
    "- Place this script at the project root (next to the `Datasets/` folder)\n",
    "- Requires: pandas, pyarrow\n",
    "    pip install pandas pyarrow\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "# How many rows to KEEP (sample) per dataset?\n",
    "#   - None ‚Üí do NOT sample/size-reduce\n",
    "#   - int (e.g., 50_000) ‚Üí sample that many rows and OVERWRITE the file\n",
    "N_TRAIN_AI2_ARC: Optional[int]  = 1000      # train-ai2_arc.parquet\n",
    "N_TRAIN_BOOLQ: Optional[int]    = 1000      # train-boolq.parquet\n",
    "N_TRAIN_SQUADV2: Optional[int]  = 1000      # train-squad_v2.parquet\n",
    "N_TRAIN_OPENMATH: Optional[int] = 1000      # train-OpenMathInstruct-2.parquet\n",
    "\n",
    "# Persist column rename even when not sampling?\n",
    "PERSIST_RENAME_IF_CHANGED: bool = True\n",
    "\n",
    "# Paths\n",
    "DATASETS_DIR = Path(\"../Datasets\")\n",
    "\n",
    "# Map file name -> sampling size\n",
    "targets = {\n",
    "    \"train-ai2_arc.parquet\": N_TRAIN_AI2_ARC,\n",
    "    \"train-boolq.parquet\": N_TRAIN_BOOLQ,\n",
    "    \"train-squad_v2.parquet\": N_TRAIN_SQUADV2,\n",
    "    \"train-OpenMathInstruct-2.parquet\": N_TRAIN_OPENMATH,\n",
    "}\n",
    "\n",
    "def load_parquet(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read a parquet file into a DataFrame.\"\"\"\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def atomic_overwrite(df: pd.DataFrame, path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Write to a temporary file and then replace the original.\n",
    "    This reduces the chance of a corrupted file if the process is interrupted.\n",
    "    \"\"\"\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    df.to_parquet(tmp, index=False)\n",
    "    tmp.replace(path)  # atomic on most OSes; fine for our use case\n",
    "\n",
    "def rename_problem_to_question(df: pd.DataFrame, file_name: str) -> Tuple[pd.DataFrame, bool]:\n",
    "    \"\"\"\n",
    "    Safely rename 'problem' column to 'question' if present.\n",
    "    Returns (df, renamed_flag).\n",
    "    - If 'question' already exists, skip to avoid duplicates.\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    if \"problem\" in cols:\n",
    "        if \"question\" in cols:\n",
    "            print(f\"‚ö†Ô∏è  '{file_name}': found both 'problem' and 'question'. Skipping rename to avoid duplicates.\")\n",
    "            return df, False\n",
    "        print(f\"üî§ Renaming column in '{file_name}': problem ‚Üí question\")\n",
    "        return df.rename(columns={\"problem\": \"question\"}), True\n",
    "    return df, False\n",
    "\n",
    "def process(file_name: str, n_sample: Optional[int]) -> None:\n",
    "    \"\"\"Print dataset info, show a small sample, and optionally overwrite with a reduced sample.\"\"\"\n",
    "    path = DATASETS_DIR / file_name\n",
    "    if not path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Missing file: {path}\")\n",
    "        return\n",
    "\n",
    "    df = load_parquet(path)\n",
    "\n",
    "    # Rename 'problem' -> 'question' if applicable (safe)\n",
    "    df, renamed = rename_problem_to_question(df, file_name)\n",
    "\n",
    "    rows, cols = df.shape\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"üìÑ Dataset: {file_name}\")\n",
    "    print(f\"üî¢ Rows: {rows:,} | Columns: {cols}\")\n",
    "\n",
    "    # Show a quick random sample (up to 5 rows, fixed seed for reproducibility)\n",
    "    k = min(5, rows)\n",
    "    print(\"\\nüß™ Sample:\")\n",
    "    print(df.sample(n=k, random_state=42))\n",
    "\n",
    "    # Overwrite logic\n",
    "    if isinstance(n_sample, int) and n_sample > 0:\n",
    "        take = min(n_sample, rows)\n",
    "        df_small = df.sample(n=take, random_state=42).reset_index(drop=True)  # already with renamed columns\n",
    "        atomic_overwrite(df_small, path)\n",
    "        print(f\"\\nüíæ Overwritten (sampled): {path} with {take} rows.\")\n",
    "    elif renamed and PERSIST_RENAME_IF_CHANGED:\n",
    "        # Persist the rename even if no sampling is requested\n",
    "        atomic_overwrite(df, path)\n",
    "        print(f\"\\nüíæ Overwritten (rename only): {path} with {rows} rows (no sampling).\")\n",
    "    else:\n",
    "        print(\"\\n‚è≠Ô∏è  Skipped overwrite (no sampling and no rename to persist).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fname, n in targets.items():\n",
    "        process(fname, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5e42c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded tokenizer: Qwen/Qwen3-0.6B\n",
      "‚úÖ Loaded tokenizer: Qwen/Qwen3-0.6B\n",
      "‚úÖ Loaded tokenizer: Qwen/Qwen3-0.6B\n",
      "‚úÖ Loaded tokenizer: Qwen/Qwen3-0.6B\n",
      "‚úÖ Loaded tokenizer: Qwen/Qwen3-0.6B\n",
      "‚úÖ Loaded tokenizer: Qwen/Qwen3-0.6B\n",
      "‚úÖ Loaded tokenizer: Qwen/Qwen3-0.6B\n",
      "‚úÖ Loaded tokenizer: Qwen/Qwen3-0.6B\n",
      "‚úÖ Analysis complete. Recommended `max_length` per dataset (cover_95):\n",
      " ‚Ä¢ [test-ai2_arc.parquet] max_length ‚âà 256  (covers ~100.0% | p95=141 | cols=question, answer)\n",
      " ‚Ä¢ [test-boolq.parquet] max_length ‚âà 128  (covers ~100.0% | p95=51 | cols=question, answer)\n",
      " ‚Ä¢ [test-OpenMathInstruct-2.parquet] max_length ‚âà 256  (covers ~98.0% | p95=188 | cols=question, answer)\n",
      " ‚Ä¢ [test-squad_v2.parquet] max_length ‚âà 128  (covers ~100.0% | p95=60 | cols=question, answer)\n",
      " ‚Ä¢ [train-ai2_arc.parquet] max_length ‚âà 256  (covers ~100.0% | p95=138 | cols=question, answer)\n",
      " ‚Ä¢ [train-boolq.parquet] max_length ‚âà 128  (covers ~100.0% | p95=51 | cols=question, answer)\n",
      " ‚Ä¢ [train-OpenMathInstruct-2.parquet] max_length ‚âà 256  (covers ~98.9% | p95=161 | cols=question, answer)\n",
      " ‚Ä¢ [train-squad_v2.parquet] max_length ‚âà 128  (covers ~100.0% | p95=63 | cols=question, answer)\n",
      "\n",
      "Detailed table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>rows_scanned</th>\n",
       "      <th>min</th>\n",
       "      <th>p50</th>\n",
       "      <th>p90</th>\n",
       "      <th>p95</th>\n",
       "      <th>p98</th>\n",
       "      <th>p99</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>coverage_90_%</th>\n",
       "      <th>suggest_cover_95</th>\n",
       "      <th>coverage_95_%</th>\n",
       "      <th>suggest_cover_98</th>\n",
       "      <th>coverage_98_%</th>\n",
       "      <th>suggest_near_max</th>\n",
       "      <th>coverage_near_max_%</th>\n",
       "      <th>recommended_max_length</th>\n",
       "      <th>template_overhead</th>\n",
       "      <th>round_multiple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test-OpenMathInstruct-2.parquet</td>\n",
       "      <td>200</td>\n",
       "      <td>55</td>\n",
       "      <td>93</td>\n",
       "      <td>152</td>\n",
       "      <td>188</td>\n",
       "      <td>231</td>\n",
       "      <td>327</td>\n",
       "      <td>571</td>\n",
       "      <td>107.645</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>256</td>\n",
       "      <td>98.0</td>\n",
       "      <td>256</td>\n",
       "      <td>98.0</td>\n",
       "      <td>640</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test-ai2_arc.parquet</td>\n",
       "      <td>200</td>\n",
       "      <td>56</td>\n",
       "      <td>92</td>\n",
       "      <td>128</td>\n",
       "      <td>141</td>\n",
       "      <td>165</td>\n",
       "      <td>192</td>\n",
       "      <td>226</td>\n",
       "      <td>96.830</td>\n",
       "      <td>...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>256</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test-boolq.parquet</td>\n",
       "      <td>200</td>\n",
       "      <td>46</td>\n",
       "      <td>47</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>54</td>\n",
       "      <td>47.835</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test-squad_v2.parquet</td>\n",
       "      <td>200</td>\n",
       "      <td>42</td>\n",
       "      <td>50</td>\n",
       "      <td>57</td>\n",
       "      <td>60</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>65</td>\n",
       "      <td>50.925</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train-OpenMathInstruct-2.parquet</td>\n",
       "      <td>1000</td>\n",
       "      <td>50</td>\n",
       "      <td>93</td>\n",
       "      <td>141</td>\n",
       "      <td>161</td>\n",
       "      <td>196</td>\n",
       "      <td>265</td>\n",
       "      <td>476</td>\n",
       "      <td>100.785</td>\n",
       "      <td>...</td>\n",
       "      <td>98.9</td>\n",
       "      <td>256</td>\n",
       "      <td>98.9</td>\n",
       "      <td>256</td>\n",
       "      <td>98.9</td>\n",
       "      <td>512</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train-ai2_arc.parquet</td>\n",
       "      <td>1000</td>\n",
       "      <td>58</td>\n",
       "      <td>91</td>\n",
       "      <td>123</td>\n",
       "      <td>138</td>\n",
       "      <td>153</td>\n",
       "      <td>161</td>\n",
       "      <td>216</td>\n",
       "      <td>94.463</td>\n",
       "      <td>...</td>\n",
       "      <td>92.7</td>\n",
       "      <td>256</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train-boolq.parquet</td>\n",
       "      <td>1000</td>\n",
       "      <td>46</td>\n",
       "      <td>48</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>53</td>\n",
       "      <td>54</td>\n",
       "      <td>61</td>\n",
       "      <td>47.964</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train-squad_v2.parquet</td>\n",
       "      <td>1000</td>\n",
       "      <td>41</td>\n",
       "      <td>51</td>\n",
       "      <td>59</td>\n",
       "      <td>63</td>\n",
       "      <td>67</td>\n",
       "      <td>76</td>\n",
       "      <td>100</td>\n",
       "      <td>52.023</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            dataset  rows_scanned  min  p50  p90  p95  p98  \\\n",
       "0   test-OpenMathInstruct-2.parquet           200   55   93  152  188  231   \n",
       "1              test-ai2_arc.parquet           200   56   92  128  141  165   \n",
       "2                test-boolq.parquet           200   46   47   50   51   52   \n",
       "3             test-squad_v2.parquet           200   42   50   57   60   63   \n",
       "4  train-OpenMathInstruct-2.parquet          1000   50   93  141  161  196   \n",
       "5             train-ai2_arc.parquet          1000   58   91  123  138  153   \n",
       "6               train-boolq.parquet          1000   46   48   50   51   53   \n",
       "7            train-squad_v2.parquet          1000   41   51   59   63   67   \n",
       "\n",
       "   p99  max     mean  ...  coverage_90_% suggest_cover_95  coverage_95_%  \\\n",
       "0  327  571  107.645  ...           98.0              256           98.0   \n",
       "1  192  226   96.830  ...           90.0              256          100.0   \n",
       "2   52   54   47.835  ...          100.0              128          100.0   \n",
       "3   63   65   50.925  ...          100.0              128          100.0   \n",
       "4  265  476  100.785  ...           98.9              256           98.9   \n",
       "5  161  216   94.463  ...           92.7              256          100.0   \n",
       "6   54   61   47.964  ...          100.0              128          100.0   \n",
       "7   76  100   52.023  ...          100.0              128          100.0   \n",
       "\n",
       "   suggest_cover_98  coverage_98_%  suggest_near_max  coverage_near_max_%  \\\n",
       "0               256           98.0               640                100.0   \n",
       "1               256          100.0               256                100.0   \n",
       "2               128          100.0               128                100.0   \n",
       "3               128          100.0               128                100.0   \n",
       "4               256           98.9               512                100.0   \n",
       "5               256          100.0               256                100.0   \n",
       "6               128          100.0               128                100.0   \n",
       "7               128          100.0               128                100.0   \n",
       "\n",
       "   recommended_max_length  template_overhead  round_multiple  \n",
       "0                     256                 32             128  \n",
       "1                     256                 32             128  \n",
       "2                     128                 32             128  \n",
       "3                     128                 32             128  \n",
       "4                     256                 32             128  \n",
       "5                     256                 32             128  \n",
       "6                     128                 32             128  \n",
       "7                     128                 32             128  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Jupyter helper ‚Äî suggest `max_length` per dataset by measuring tokenized lengths.\n",
    "\n",
    "What it does\n",
    "- Auto-discovers ../Datasets/*.parquet (both train-*.parquet and test-*.parquet)\n",
    "- Guesses relevant text columns (question/context/etc.). You can override.\n",
    "- Tokenizes with your model‚Äôs tokenizer; falls back to char-based heuristic if needed.\n",
    "- Reports p50/p90/p95/p98/p99/max and suggested max_length values (cover_90/95/98/near_max).\n",
    "\n",
    "How to use\n",
    "- Just run this cell. Optionally adjust the CONFIG block.\n",
    "\"\"\"\n",
    "\n",
    "# ========================\n",
    "# CONFIG (edit as needed)\n",
    "# ========================\n",
    "from pathlib import Path\n",
    "\n",
    "DATASETS_DIR = Path(\"../Datasets\")\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"   # tokenizer name\n",
    "SAMPLE_MAX = 30_000              # max rows to analyze per dataset (random sample)\n",
    "TEMPLATE_OVERHEAD = 32           # tokens added by your prompt template/system\n",
    "ROUND_MULTIPLE = 128             # round suggestions up to this multiple\n",
    "TEXT_COLS_OVERRIDE = None        # e.g., [\"question\",\"context\",\"choices\",\"response\"] or None to auto\n",
    "GLOB_PATTERN = \"*-*.parquet\"     # which files to analyze inside DATASETS_DIR\n",
    "\n",
    "# ========================\n",
    "# Implementation\n",
    "# ========================\n",
    "import math\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def guess_text_columns(df: pd.DataFrame):\n",
    "    # Prioritized list of likely text-bearing columns\n",
    "    candidates = [\n",
    "        \"question\",\"answer\",\n",
    "    ]\n",
    "    present = [c for c in candidates if c in df.columns]\n",
    "    if not present:\n",
    "        present = [c for c in df.columns if pd.api.types.is_string_dtype(df[c])]\n",
    "    return present[:6]\n",
    "\n",
    "def _to_list_if_serialized(obj):\n",
    "    if isinstance(obj, list):\n",
    "        return obj\n",
    "    if isinstance(obj, str):\n",
    "        s = obj.strip()\n",
    "        if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "            try:\n",
    "                return list(ast.literal_eval(s))\n",
    "            except Exception:\n",
    "                return [obj]\n",
    "    return [obj]\n",
    "\n",
    "def _row_to_text(row, cols):\n",
    "    parts = []\n",
    "    for c in cols:\n",
    "        if c not in row or pd.isna(row[c]):\n",
    "            continue\n",
    "        val = row[c]\n",
    "        if c == \"choices\":\n",
    "            items = _to_list_if_serialized(val)\n",
    "            parts.append(\"Choices:\\n- \" + \"\\n- \".join(str(x) for x in items))\n",
    "        else:\n",
    "            parts.append(f\"{c.capitalize()}: {val}\")\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "def _build_texts(df, cols):\n",
    "    for _, r in df[cols].iterrows():\n",
    "        yield _row_to_text(r, cols)\n",
    "\n",
    "def load_token_counter(model_name):\n",
    "    # Try HF tokenizer; if it fails (no internet/cache), fall back to a chars-per-token rule\n",
    "    try:\n",
    "        from transformers import AutoTokenizer\n",
    "        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True)\n",
    "        def count_tokens(text: str) -> int:\n",
    "            return len(tok(text, add_special_tokens=True,\n",
    "                           return_attention_mask=False,\n",
    "                           return_token_type_ids=False)[\"input_ids\"])\n",
    "        print(f\"‚úÖ Loaded tokenizer: {model_name}\")\n",
    "        return count_tokens\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load tokenizer ({e}). Using char-based estimate.\")\n",
    "        avg_chars_per_token = 3.7\n",
    "        def count_tokens(text: str) -> int:\n",
    "            return int(math.ceil(len(text) / avg_chars_per_token))\n",
    "        return count_tokens\n",
    "\n",
    "def round_up(x: float, base: int = 128) -> int:\n",
    "    return int(base * math.ceil(x / base))\n",
    "\n",
    "def analyze_dataset(path: Path, model_name: str, text_cols_override=None,\n",
    "                    sample_max: int = 30_000, template_overhead: int = 32,\n",
    "                    round_multiple: int = 128):\n",
    "    df = pd.read_parquet(path)\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(f\"{path.name} is empty.\")\n",
    "\n",
    "    cols = text_cols_override or guess_text_columns(df)\n",
    "    if not cols:\n",
    "        raise ValueError(f\"Could not guess text columns for {path.name}. Set TEXT_COLS_OVERRIDE.\")\n",
    "\n",
    "    if sample_max and len(df) > sample_max:\n",
    "        df = df.sample(n=sample_max, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    count_tokens = load_token_counter(model_name)\n",
    "    texts = list(_build_texts(df, cols))\n",
    "    lengths = np.fromiter((count_tokens(t) + template_overhead for t in texts), dtype=np.int32)\n",
    "\n",
    "    pct = {p: int(np.percentile(lengths, p)) for p in [50, 90, 95, 98, 99]}\n",
    "    stats = {\n",
    "        \"dataset\": path.name,\n",
    "        \"rows_scanned\": len(lengths),\n",
    "        \"min\": int(lengths.min()),\n",
    "        \"p50\": pct[50],\n",
    "        \"p90\": pct[90],\n",
    "        \"p95\": pct[95],\n",
    "        \"p98\": pct[98],\n",
    "        \"p99\": pct[99],\n",
    "        \"max\": int(lengths.max()),\n",
    "        \"mean\": float(lengths.mean()),\n",
    "        \"std\": float(lengths.std()),\n",
    "        \"cols_used\": \", \".join(cols),\n",
    "    }\n",
    "\n",
    "    suggestions = {\n",
    "        \"cover_90\": round_up(stats[\"p90\"], round_multiple),\n",
    "        \"cover_95\": round_up(stats[\"p95\"], round_multiple),\n",
    "        \"cover_98\": round_up(stats[\"p98\"], round_multiple),\n",
    "        \"near_max\": round_up(stats[\"max\"], round_multiple),\n",
    "    }\n",
    "\n",
    "    def coverage(L): return float((lengths <= L).mean())\n",
    "\n",
    "    rec = {\n",
    "        \"suggest_cover_90\": suggestions[\"cover_90\"],\n",
    "        \"coverage_90_%\": round(coverage(suggestions[\"cover_90\"]) * 100, 1),\n",
    "        \"suggest_cover_95\": suggestions[\"cover_95\"],\n",
    "        \"coverage_95_%\": round(coverage(suggestions[\"cover_95\"]) * 100, 1),\n",
    "        \"suggest_cover_98\": suggestions[\"cover_98\"],\n",
    "        \"coverage_98_%\": round(coverage(suggestions[\"cover_98\"]) * 100, 1),\n",
    "        \"suggest_near_max\": suggestions[\"near_max\"],\n",
    "        \"coverage_near_max_%\": round(coverage(suggestions[\"near_max\"]) * 100, 1),\n",
    "        \"recommended_max_length\": suggestions[\"cover_95\"],  # default recommendation\n",
    "        \"template_overhead\": template_overhead,\n",
    "        \"round_multiple\": round_multiple,\n",
    "    }\n",
    "\n",
    "    # Pretty one-line recommendation for copy-paste\n",
    "    pretty = (\n",
    "        f\"[{path.name}] max_length ‚âà {rec['recommended_max_length']}  \"\n",
    "        f\"(covers ~{rec['coverage_95_%']}% | p95={stats['p95']} | cols={stats['cols_used']})\"\n",
    "    )\n",
    "\n",
    "    return stats, rec, pretty\n",
    "\n",
    "# Run over discovered datasets and show a table\n",
    "paths = sorted(DATASETS_DIR.glob(GLOB_PATTERN))\n",
    "if not paths:\n",
    "    raise FileNotFoundError(f\"No datasets matched {DATASETS_DIR / GLOB_PATTERN}\")\n",
    "\n",
    "all_rows = []\n",
    "pretties = []\n",
    "for p in paths:\n",
    "    stats, rec, pretty = analyze_dataset(\n",
    "        p,\n",
    "        model_name=MODEL_NAME,\n",
    "        text_cols_override=TEXT_COLS_OVERRIDE,\n",
    "        sample_max=SAMPLE_MAX,\n",
    "        template_overhead=TEMPLATE_OVERHEAD,\n",
    "        round_multiple=ROUND_MULTIPLE,\n",
    "    )\n",
    "    all_rows.append({**stats, **rec})\n",
    "    pretties.append(pretty)\n",
    "\n",
    "res_df = pd.DataFrame(all_rows).sort_values(\"dataset\").reset_index(drop=True)\n",
    "\n",
    "# Display results\n",
    "from IPython.display import display\n",
    "print(\"‚úÖ Analysis complete. Recommended `max_length` per dataset (cover_95):\")\n",
    "for line in pretties:\n",
    "    print(\" ‚Ä¢\", line)\n",
    "\n",
    "print(\"\\nDetailed table:\")\n",
    "display(res_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
