# Agent Instructions: Advanced Quantization Experiment

**Branch**: `advanced-quantization-llama`  
**Phase**: 4 - Advanced Quantization Methods Comparison  
**Model**: `meta-llama/Llama-3.2-1B-Instruct` (FIXED - do not change)  
**Platform**: WSL/Linux (ALWAYS - never use Windows directly)

---

## Core Principles

### üî¥ ALWAYS Rules (Never Break These)

1. **ALWAYS use WSL/Linux environment**
   - Verify with `uname -a` before starting any work
   - If not in WSL, stop and switch to WSL first
   - Use Linux paths (`/mnt/c/...`) not Windows paths

2. **ALWAYS use `meta-llama/Llama-3.2-1B-Instruct`**
   - Never substitute with another model
   - This is the fixed baseline for all experiments

3. **ALWAYS test quantized models before proceeding**
   - Run sanity check (basic generation)
   - Run full evaluation (3 datasets)
   - Save results before moving to next method

4. **ALWAYS save metadata**
   - Quantization config (JSON)
   - Method parameters
   - Library versions
   - Quantization time
   - Model size before/after

5. **ALWAYS compare to baseline**
   - Calculate accuracy degradation
   - Check model size reduction
   - Compare latency
   - Document in comparison report

### üü¢ SHOULD Rules (Follow Unless Good Reason)

1. **SHOULD prioritize mature methods first**
   - Order: GPTQ ‚Üí AWQ ‚Üí HQQ ‚Üí SmoothQuant ‚Üí AdaRound ‚Üí QuaRot
   - Get working solutions before experimental ones

2. **SHOULD use standard configs initially**
   - Don't over-optimize early
   - Use recommended defaults from papers/libraries
   - Document deviations

3. **SHOULD isolate errors per method**
   - If one method fails, continue with others
   - Document failures clearly
   - Don't let one failure block entire experiment

4. **SHOULD keep code modular**
   - One script per quantization method
   - Shared utilities in common module
   - Easy to run independently

---

## Project Structure

```
experiments/advanced-quantization/
‚îú‚îÄ‚îÄ README.md                      # Experiment overview
‚îú‚îÄ‚îÄ common/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ baseline.py                # Baseline model utilities
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py              # Shared evaluation functions
‚îÇ   ‚îú‚îÄ‚îÄ metadata.py                # Metadata saving/loading
‚îÇ   ‚îî‚îÄ‚îÄ verification.py            # Model verification checks
‚îÇ
‚îú‚îÄ‚îÄ quantize_gptq.py              # GPTQ quantization
‚îú‚îÄ‚îÄ quantize_awq.py               # AWQ quantization
‚îú‚îÄ‚îÄ quantize_hqq.py               # HQQ quantization
‚îú‚îÄ‚îÄ quantize_smoothquant.py       # SmoothQuant quantization
‚îú‚îÄ‚îÄ quantize_adaround.py          # AdaRound quantization
‚îú‚îÄ‚îÄ quantize_quarot.py            # QuaRot quantization
‚îÇ
‚îú‚îÄ‚îÄ verify_model.py               # Quick model verification script
‚îú‚îÄ‚îÄ compare_to_baseline.py        # Comparison utility
‚îú‚îÄ‚îÄ run_all_quantization.sh       # Master orchestration (bash)
‚îî‚îÄ‚îÄ analyze_results.py            # Results analysis and visualization
```

---

## Standard Workflow

### For Each Quantization Method

```bash
# 1. Verify environment
./experiments/advanced-quantization/verify_env.sh

# 2. Quantize model
python experiments/advanced-quantization/quantize_<method>.py \
  --model meta-llama/Llama-3.2-1B-Instruct \
  --bits 4 \
  --output Models/Llama-3.2-1B-Instruct/<method>-4bit \
  --save-metadata

# 3. Quick verification
python experiments/advanced-quantization/verify_model.py \
  --model-path Models/Llama-3.2-1B-Instruct/<method>-4bit \
  --prompt "What is 2+2?"

# 4. Full evaluation
python Testing/02_TestModels.py \
  --model-path Models/Llama-3.2-1B-Instruct/<method>-4bit \
  --output Testing/metrics/advanced-quantization/

# 5. Compare to baseline
python experiments/advanced-quantization/compare_to_baseline.py \
  --baseline Testing/metrics/advanced-quantization/llama-3.2-1b-baseline.json \
  --quantized Testing/metrics/advanced-quantization/llama-3.2-1b-<method>-4bit.json \
  --output experiments/advanced-quantization/comparisons/<method>-4bit.md

# 6. Document results
# Update experiments/advanced-quantization/README.md with results
```

---

## Script Templates

### Standard Quantization Script Structure

Every `quantize_<method>.py` should follow this structure:

```python
#!/usr/bin/env python3
"""
Quantize Llama-3.2-1B-Instruct using <METHOD>.

Usage:
    python quantize_<method>.py --bits 4 --output ./output
"""

import argparse
import json
import time
from pathlib import Path
from datetime import datetime
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Method-specific imports
from <library> import ...

def verify_wsl():
    """Verify we're running in WSL."""
    import platform
    if platform.system() != "Linux":
        raise RuntimeError("‚ùå Must run in WSL/Linux! Current: " + platform.system())
    print("‚úÖ Running in Linux/WSL")

def load_model(model_name: str):
    """Load the baseline model."""
    print(f"Loading model: {model_name}")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer

def quantize_model(model, tokenizer, args):
    """Quantize the model using <METHOD>."""
    print(f"Quantizing with <METHOD> ({args.bits}-bit)...")
    
    start_time = time.time()
    
    # Method-specific quantization logic here
    # ...
    
    quantization_time = time.time() - start_time
    
    print(f"‚úÖ Quantization completed in {quantization_time:.2f}s")
    return quantized_model, quantization_time

def save_metadata(output_path: Path, args, quantization_time: float, model_size_before: int, model_size_after: int):
    """Save quantization metadata."""
    metadata = {
        "method": "<METHOD>",
        "model": "meta-llama/Llama-3.2-1B-Instruct",
        "bits": args.bits,
        "quantization_time_seconds": quantization_time,
        "model_size_before_mb": model_size_before / (1024 * 1024),
        "model_size_after_mb": model_size_after / (1024 * 1024),
        "compression_ratio": model_size_before / model_size_after,
        "timestamp": datetime.now().isoformat(),
        "config": vars(args)
    }
    
    metadata_path = output_path / "quantization_metadata.json"
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=2)
    
    print(f"‚úÖ Metadata saved to {metadata_path}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", default="meta-llama/Llama-3.2-1B-Instruct")
    parser.add_argument("--bits", type=int, required=True, choices=[4, 8])
    parser.add_argument("--output", type=str, required=True)
    parser.add_argument("--save-metadata", action="store_true")
    # Add method-specific arguments
    
    args = parser.parse_args()
    
    # Verify WSL
    verify_wsl()
    
    # Verify model is correct
    if args.model != "meta-llama/Llama-3.2-1B-Instruct":
        raise ValueError(f"‚ùå Wrong model! Must use meta-llama/Llama-3.2-1B-Instruct, got: {args.model}")
    
    output_path = Path(args.output)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Load model
    model, tokenizer = load_model(args.model)
    
    # Get size before
    model_size_before = sum(p.numel() * p.element_size() for p in model.parameters())
    
    # Quantize
    quantized_model, quantization_time = quantize_model(model, tokenizer, args)
    
    # Save quantized model
    print(f"Saving quantized model to {output_path}")
    quantized_model.save_pretrained(output_path)
    tokenizer.save_pretrained(output_path)
    
    # Get size after
    model_size_after = sum(p.numel() * p.element_size() for p in quantized_model.parameters())
    
    # Save metadata
    if args.save_metadata:
        save_metadata(output_path, args, quantization_time, model_size_before, model_size_after)
    
    print("‚úÖ Done!")

if __name__ == "__main__":
    main()
```

---

## Verification Checklist

Before considering a quantization method "complete":

- [ ] Script runs without errors
- [ ] Quantized model saves successfully
- [ ] Model size reduced as expected (~25% for 4-bit, ~50% for 8-bit)
- [ ] Model loads and generates text
- [ ] Basic sanity check passes (coherent output)
- [ ] Full evaluation completes on all 3 datasets
- [ ] Metadata JSON saved correctly
- [ ] Comparison to baseline generated
- [ ] Results documented in experiment README

---

## Error Handling

### If Quantization Fails

1. **Check error message carefully**
2. **Try with CPU** (if GPU OOM):
   ```bash
   python quantize_<method>.py --device cpu ...
   ```
3. **Reduce calibration dataset size**:
   ```bash
   python quantize_<method>.py --calib-samples 128 ...  # Default 512
   ```
4. **Check library compatibility**:
   ```bash
   pip show <library>  # Check version
   ```
5. **Document the failure**:
   - Error message
   - System specs
   - Attempted fixes
   - Mark method as "failed" in results

### If Evaluation Fails

1. **Check model loads correctly**
2. **Test with single dataset first**
3. **Check for OOM issues** (reduce batch size)
4. **Verify dataset paths correct**
5. **Document partial results** if some datasets work

### If Comparison Shows Poor Results

1. **Verify baseline is correct**
2. **Check quantization config** (maybe too aggressive)
3. **Try different parameters** (group size, calibration data)
4. **Document trade-offs** (accuracy vs size)
5. **Note if method unsuitable** for this model/task

---

## Key Metrics to Track

### Per Quantized Model

**Quantization Metrics**:
- Quantization time (seconds)
- Model size before (MB)
- Model size after (MB)
- Compression ratio (before/after)
- Peak memory during quantization (GB)

**Accuracy Metrics** (per dataset):
- ARC: Macro F1, accuracy
- SQuAD: F1, Exact Match
- OpenMath: Mean Absolute Difference

**Efficiency Metrics**:
- Inference latency per sample (seconds)
- Tokens/second generation rate
- VRAM usage during inference (GB)

**Degradation Metrics**:
- Accuracy drop vs baseline (%)
- Latency change vs baseline (%)
- Model size reduction (%)

---

## Communication Guidelines

### When Reporting Progress

‚úÖ **Good**:
```
‚úÖ GPTQ 4-bit complete
- Quantization time: 8.3 minutes
- Model size: 257MB ‚Üí 68MB (26.4% of original)
- ARC F1: 0.847 (-3.2% vs baseline)
- SQuAD F1: 61.3 (-2.1% vs baseline)
- OpenMath AbsDiff: 12.4 (+38% vs baseline)
- Verdict: Good compression, acceptable accuracy drop
```

‚ùå **Bad**:
```
GPTQ done, looks ok
```

### When Reporting Errors

‚úÖ **Good**:
```
‚ùå QuaRot quantization failed
- Error: RuntimeError: CUDA out of memory
- Attempted: GPU quantization with default config
- Fix tried: CPU fallback ‚Üí still failed (compilation issue)
- Status: Marking as "implementation unavailable"
- Recommendation: Skip for this experiment, document why
```

‚ùå **Bad**:
```
QuaRot doesn't work
```

---

## Final Deliverables

At end of experiment, ensure:

1. **Code**:
   - [ ] All quantization scripts completed
   - [ ] Shared utilities in `common/`
   - [ ] Master orchestration script works
   - [ ] Code documented and clean

2. **Models**:
   - [ ] All quantized models saved in `Models/`
   - [ ] Metadata JSON for each model
   - [ ] Baseline model evaluated

3. **Results**:
   - [ ] All evaluation JSONs in `Testing/metrics/advanced-quantization/`
   - [ ] Comparison reports generated
   - [ ] Summary CSV with all results

4. **Documentation**:
   - [ ] `experiments/advanced-quantization/README.md` complete
   - [ ] `docs/Phase4_Advanced_Quantization.md` written
   - [ ] Main README updated with Phase 4 summary
   - [ ] All methods documented (including failures)

5. **Analysis**:
   - [ ] Comparison tables generated
   - [ ] Visualizations created (plots)
   - [ ] Trade-off analysis documented
   - [ ] Recommendations provided

---

## Quick Reference Commands

### Environment Check
```bash
# Verify WSL
uname -a

# Check CUDA
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"

# Check disk space
df -h

# Check memory
free -h
```

### Running Experiments
```bash
# CD to project root
cd /mnt/c/Users/AlbertoTC/Documents/code/LLM-Training

# Activate environment (if needed)
# source venv/bin/activate

# Run single method
python experiments/advanced-quantization/quantize_gptq.py --bits 4 --output Models/Llama-3.2-1B-Instruct/gptq-4bit

# Run all methods (when ready)
bash experiments/advanced-quantization/run_all_quantization.sh
```

### Debugging
```bash
# Check model loads
python -c "from transformers import AutoModelForCausalLM; m = AutoModelForCausalLM.from_pretrained('Models/Llama-3.2-1B-Instruct/gptq-4bit'); print('OK')"

# Test generation
python experiments/advanced-quantization/verify_model.py --model-path Models/Llama-3.2-1B-Instruct/gptq-4bit

# Check file sizes
du -sh Models/Llama-3.2-1B-Instruct/*
```

---

## Remember

- **Think systematically**: One method at a time, verify each step
- **Document everything**: Success, failure, attempts, results
- **Compare constantly**: Always reference baseline
- **Be pragmatic**: If a method doesn't work after reasonable effort, document and move on
- **Focus on value**: The goal is a comprehensive comparison, not implementing every method perfectly

---

**Last Updated**: November 28, 2025  
**Branch**: `advanced-quantization-llama`  
**Status**: Ready to begin implementation
